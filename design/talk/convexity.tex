
\documentclass[10pt]{beamer}
%\beamertemplateshadingbackground{brown!70}{yellow!10}
\mode<presentation>
{
  %\usetheme{Warsaw}
  \usecolortheme{crane}
  % or ...

%  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]{}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
\usepackage[utf8]{inputenc}
\usetikzlibrary{patterns}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{colortbl}
%\usepackage{multicol}
\usepackage{cancel}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{relsize}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{forloop}% http://ctan.org/pkg/forloop
\newcounter{loopcntr}
\newcommand{\rpt}[2][1]{%
  \forloop{loopcntr}{0}{\value{loopcntr}<#1}{#2}%
}
%\pagestyle{plain}
%\input{defs2}
\input{../shortdefs}

\edef\polishl{\l}
\setlength{\columnsep}{0.7em}
\setlength{\columnseprule}{0mm}
\setlength{\arrayrulewidth}{1pt} 

\newcommand{\svr}[1]{{\textcolor{darkSilver}{#1}}}
\definecolor{brightyellow}{cmyk}{0,0,0.7,0.0}
\definecolor{lightyellow}{cmyk}{0,0,0.3,0.0}
\definecolor{lighteryellow}{cmyk}{0,0,0.1,0.0}
\definecolor{lightestyellow}{cmyk}{0,0,0.05,0.0}
\AtBeginSection[]
{
\begin{frame}<beamer>
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}
}
\def\layersep{2.5cm}


%  \fboxsep=3pt
% %\fboxsep=0mm%padding thickness
% \fboxrule=2pt%border thickness

\setkeys{Gin}{width=0.7\textwidth}

\title[]{Convexity conjectures for combinatorial optimization}

\author[]{Micha{\l} Derezi\'{n}ski\\
UC Berkeley}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

\linespread{1.3}

\begin{frame}
\frametitle{General strategy for combinatorial optimization}
We consider the following minimization problem:
\begin{align*}
  \opt=\min_{S\subseteq[n]}f(S)\quad\text{subject to}\quad |S|=k.
\end{align*}
\pause
Suppose that an unnormalized distribution $\mu:2^{[n]}\rightarrow\R_{\geq 0}$
satisfies:
\begin{align*}
  \E_{S\sim\mu}[f(S)]\leq (1+\epsilon)\cdot\opt\quad
  \text{and}\quad \E_{S\sim \mu}[|S|] = k.
\end{align*}
\pause
\vspace{-3mm}
\begin{block}{Convexity assumption}
Suppose that  the function  $g(x) = \E_{S\sim\mu}[f(S)\mid |S|=x]$
is convex.
\end{block}
\vspace{4mm}
\pause
If convexity assumption holds, then from Jensen's inequality we get:
\begin{align*}
  \opt\leq\E_{S\sim\mu}[f(S)\mid |S|=k] \leq \E_{S\sim\mu}[f(S)]
  \leq(1+\epsilon)\cdot\opt.
\end{align*}
\end{frame}

\section{Column Subset Selection Problem}

\begin{frame}
  \frametitle{CSSP using Determinantal Point Processes (DPPs)}
  For a matrix $\A\in\R^{m\times n}$, we define:
  \begin{align*}
    f(S) := \|\A - \P_S\A\|_F^2\quad\text{and}\quad\mu(S) := \det(\tfrac1\alpha\A_S^\top\A_S).
  \end{align*}
  Then, we have:
  \begin{align*}
    \E_{S\sim\mu}[f(S)] = \alpha\cdot \E[|S|]
    \quad\text{and}\quad
    \E[|S|] = \tr(\A^\top\A(\alpha\I+\A^\top\A)^{-1}).
  \end{align*}
  \pause
  Let $t_s=s+\sigma_{s+1}^{-2}\sum_{i>s}\sigma_i^2$ be a soft
  rank of $\A$, where $\sigma_i$ are the singular values and $s\geq 0$. Then for $s<k<t_s$, there is $\alpha$ such that:
  \begin{align*}
\E_{S\sim\mu}[|S|]=k \quad\text{and}\quad \alpha k\leq \big(1+\tfrac
    s{k-s}\big)\sqrt{1+\tfrac{2(k-s)}{t_s-k}}\cdot\opt.
  \end{align*}
  \vspace{5mm}
\pause
  
  \textbf{Goal. } Establish convexity of the following function:
  \begin{align*}
  g(x) =
    (x+1)\frac{\sum_{S:|S|=x+1}\prod_{i\in
    S}\sigma_i^2}{\sum_{S:|S|=x}\prod_{i\in S}\sigma_i^2}.
  \end{align*}
  \pause
Then: 
  $\E_{S\sim\mu}[f(S)\mid |S|=k]=g(k)\leq \E_{S\sim\mu}[g(|S|)]=
    \E_{S\sim\mu}[f(S)]$
\end{frame}

\begin{frame}
  \frametitle{Main convexity conjecture}
  \textbf{Conjecture.} \\
  For any $\lambda_1,...,\lambda_n>0$, the
following function is convex over $k\in[n]$:
  \begin{align*}
g(k) = (k+1)\frac{e_{k+1}}{e_k},\quad\text{where}\quad e_k = \sum_{S:|S|=k}\prod_{i\in S}\lambda_i.
  \end{align*}
  \pause
  We can reformulate the convexity condition as follows:
  \begin{align*}
    (k+1)\frac{e_{k+1}}{e_k} + (k-1)\frac{e_{k-1}}{e_{k-2}} -
    2k\frac{e_k}{e_{k-1}}\geq 0,\quad\text{for all $k$.}
  \end{align*}
% Denoting $\Delta_k = \frac{e_k}{e_{k-1}} - \frac{e_{k+1}}{e_k}$, we
% can simplify this as follows:
% \begin{align*}
%   \Delta_{k-1} - \Delta_k\geq
%   \frac1k(\Delta_{k-1}+\Delta_k),\quad\text{for all $k$.}
% \end{align*}
% Finally, note that we can write:
% \begin{align*}
%   g(k) = (n-k)\frac{E_{k+1}}{E_k},\quad\text{where}\quad E_k =
%   \frac{e_k}{{n\choose k}}.
% \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Implications of the conjecture}
\textbf{Master Theorem} using measure concentration\\[2mm]
  Given $0\leq s<\rank(\A) $, let  $t_s = s+\sr_s(\A)$,
and suppose that $s+ \frac7{\epsilon^4}\ln^2\!\frac1\epsilon \leq k\leq t_s-1$,
where $0<\epsilon\leq\frac12$. If $S\sim k$-$\DPP(\A^\top\A)$, then
\begin{align*}
  \frac{\E[\Er_\A(S)]}{\opt}\leq (1+2\epsilon)^2\,\Phi_{s}(k),
  \quad\text{where}\quad \Phi_s(k)=\big(1+\tfrac{s}{k-s}\big)\sqrt{1 + \tfrac{2(k-s)}{t_s-k}\,}.
\end{align*}
\pause

\begin{theorem}[Master Theorem using the convexity conjecture]
  Given $0\leq s<k<t_s$, where $t_s= s+\sr_s(\A)$, if $S\sim
  k$-$\DPP(\A^\top\A)$ then:
  \begin{align*}
    \frac{\E[\Er_{\A}(S)]}{\opt} \leq \Phi_s(k).
  \end{align*}
\end{theorem}
\pause
\begin{enumerate}
\item Much tighter constant factors
\item Applies to all $k=O(1)$
\item Simpler analysis
\end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Further improvements}

  \begin{itemize}
  \item Precise constants for polynomial and exponential spectral decay
\vspace{5mm}\pause
    \item Analyze the over-sampled version of the problem:\\
      Find $r\geq k$ such that $\E_{S\sim\mu}[\Er_{\A}(S)\mid
      |S|=r] \leq (1+\epsilon)\cdot \opt$.\\
      (Worst-case analysis yields $r\geq k+k/\epsilon-1$.)
\vspace{5mm}\pause
      \item Improve the bounds by solving a convex relaxation of the problem.
  \end{itemize}
\end{frame}

\section{Optimal design of experiments}

\begin{frame}
  \frametitle{Classical experimental design}
  Consider $n$ parameterized experiments:
  $\x_1,\dots,\x_n\in\R^d$.\\
  Each experiment has a real random response $y_i$ such that:
  \begin{align*}
    y_i = \x_i^\top\w^* + \xi_i,\qquad \xi_i\sim\Nc(0,\sigma^2)
  \end{align*}
  \pause
\textbf{Goal:} Select $k\ll n$ experiments to best estimate $\w^*$
  \pause
\begin{columns}
\begin{column}{0.3\textwidth}
\\ \vspace{0.8cm}
Select $S=\{4,6,9\}$\\
\vspace{1cm}
Receive $y_4, y_6, y_9$
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
	\begin{tikzpicture}[scale=0.9]
          \draw [fill=brown!30] (-2,0) rectangle (0,3);
          \draw [color=black] (-2,2) -- (0,2);
          \draw (-2.25,2) node {\mbox{\footnotesize $\x_4^\top$}}; 
          \draw [color=black] (-2,1.5) -- (0,1.5);
          \draw (-2.25,1.5) node {\mbox{\footnotesize $\x_6^\top$}}; 
          \draw [color=black] (-2,0.5) -- (0,0.5);
          \draw (-2.25,0.5) node {\mbox{\footnotesize $\x_9^\top$}}; 
	   \draw (-3,3) node {fixed $\X$}; 
           \draw [decorate,decoration={brace}] (-2,3.1) -- (0,3.1);
          \draw (-1,3.4) node {\mbox{\fontsize{8}{8}\selectfont $d$}}; 
            \draw [color=lightgray,line width =0.5mm] (1,0) -- (1,3);
            \draw [color=lightgray] (0.75,3) node {$\y$};
            \draw (0.75,2) node {\mbox{\footnotesize $y_4$}}; 
            \draw (1,2) node {.}; 
            \draw[mark=*,mark size=1.5pt] plot coordinates{(1,2)};
            \draw (0.75,1.5) node {\mbox{\footnotesize $y_6$}}; 
            \draw (1,1.5) node {.}; 
            \draw[mark=*,mark size=1.5pt] plot coordinates{(1,1.5)};
            \draw (0.75,0.5) node {\mbox{\footnotesize $y_9$}}; 
            \draw[mark=*,mark size=1.5pt] plot coordinates{(1,.5)};
	\end{tikzpicture}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
  \frametitle{Optimal design criteria}
  \begin{enumerate}
  \item A-optimality:\quad $f(S) = F(\X_S^\top\X_S) := \tr\big((\X_S^\top\X_S)^{-1}\big)$\\
    {\footnotesize\textit{Interpretation:} Mean Squared Error}
    \pause
\item C-optimality:\quad $f(S) = F(\X_S^\top\X_S) := \cb^\top(\X_S^\top\X_S)^{-1}\cb$ for
  $\cb \in \R^d$\\
    {\footnotesize  \textit{Interpretation:} Variance in a particular
      direction}
        \pause
\item D-optimality:\quad $f(S) = F(\X_S^\top\X_S) := \det(\X_S^\top\X_S)^{-1/d}$\\
    {\footnotesize  \textit{Interpretation:} Volume of the confidence
      ellipsoid}
        \pause
  \item V-optimality:\quad $f(S) = F(\X_S^\top\X_S) :=
    \frac1n\tr\big(\X(\X_S^\top\X_S)^{-1}\X^\top\big)$\\
    {\footnotesize    \textit{Interpretation:} Mean Squared Prediction Error}
  \end{enumerate}
\vspace{5mm}
  \pause
  
  We obtain the following minimization problem:
\begin{align*}
  \opt=\min_{S\subseteq[n]}f(S)\quad\text{subject to}\quad |S|=k.
\end{align*}
\end{frame}

\begin{frame}
  \frametitle{Extension: Bayesian experimental design}
    Consider $n$ parameterized experiments:
  $\x_1,\dots,\x_n\in\R^d$.\\
  Each experiment has a real random response $y_i$ such that:
  \begin{align*}
    y_i = \x_i^\top\w^* + \xi_i,\qquad \xi_i\sim\Nc(0,\sigma^2),\quad \Blue{\w^*\sim\Nc(\zero,\sigma^2\A^{-1})}
  \end{align*}
\pause
    \begin{enumerate}
  \item A-optimality:\quad $f(S) = F(\X_S^\top\X_S+\A) := \tr\big((\X_S^\top\X_S+\A)^{-1}\big)$
\item C-optimality:\quad $f(S)= F(\X_S^\top\X_S+\A) := \cb^\top(\X_S^\top\X_S+\A)^{-1}\cb$ 
\item D-optimality:\quad $f(S) = F(\X_S^\top\X_S+\A) := \det(\X_S^\top\X_S+\A)^{-1/d}$
  \item V-optimality:\quad $f(S) = F(\X_S^\top\X_S+\A) :=
    \frac1n\tr\big(\X(\X_S^\top\X_S+\A)^{-1}\X^\top\big)$\\
  \end{enumerate}
  \pause
  \vspace{5mm}
  
  We recover classical optimal design with $\A=\zero$.  
\end{frame}

\begin{frame}
  \frametitle{Key result}
  For a matrix $\X\in\R^{n\times d}$ and a p.s.d.~matrix
  $\A\in\R^{d\times d}$, define:
  \begin{align*}
    \mu(S)=\det(\X_S^\top\X_S+\A)\cdot\prod_{i\in
    S}p_i\cdot\prod_{i\not\in S}(1-p_i),
    \quad\text{over }S\subseteq [n].
  \end{align*}
  \pause

  \begin{theorem}
  Let $f$ be A/C/D/V-optimality with precision matrix $\A$. Then:
  \begin{align*}
    \E_{S\sim\mu}\big[f(S)\big] \leq F\Big(\sum_ip_i\x_i\x_i^\top + \A\Big)
  \end{align*}
\end{theorem}
\vspace{5mm}
\pause

\emph{Application}. Randomized rounding for the following convex
relaxation:
\begin{align*}
  p^*
  &= \argmin_p F\Big(\sum_ip_i\x_i\x_i^\top+\A\Big)
\quad\text{subject to }\ 0\leq p_i\leq 1,\ \sum_ip_i=k.
\end{align*}
\end{frame}

\begin{frame}
  \frametitle{Implications of convexity}
  \textbf{Main result} using measure concentration\\[2mm]
    Let $f$ be A/C/D/V-optimality and $\X$ be $n\times d$. There is an absolute
    constant $C>0$ such that if
  $k\geq C\big(\frac{d}{\epsilon} +
  \frac{\log1/\epsilon}{\epsilon^2}\big)$, then 
we can find in polynomial time a subset $S$ of size $k$ s.t.
  \begin{align*}
    f(S) \leq (1+\epsilon)\cdot\opt.
  \end{align*}
  \pause
  
  \begin{theorem}[Main result using convexity assumption]
    Let $f$ be A/C/D/V-optimality and $\X$ be $n\times d$. If
  $k\geq d+\frac{d}{\epsilon}$, then 
we can find in polynomial time a subset $S$ of size $k$ s.t.
  \begin{align*}
    f(S) \leq (1+\epsilon)\cdot\opt.
  \end{align*}    
  \end{theorem}
  
\end{frame}


\begin{frame}
  \frametitle{Example: D-optimal design}
  For a matrix $\X\in\R^{n\times d}$ and a p.s.d.~matrix
  $\A\in\R^{d\times d}$, we define:
  \begin{align*}
   \hspace{-4mm} f(S) = \det(\X_S^\top\X_S+\A)^{-1}\quad\text{and}\quad
    \mu(S)=\det(\X_S^\top\X_S+\A)\cdot\prod_{i\in S}p_i\cdot\prod_{i\not\in S}(1-p_i).
  \end{align*}
  If $k\geq d_{\A}+\frac{d_{\A}}\epsilon$ for
  $d_{\A}=\tr(\X^\top\X(\X^\top\X+\A)^{-1})$, then there are $p_i$'s s.t.:
  \begin{align*}
    \E_{S\sim\mu}[f(S)]\leq(1+\epsilon)\cdot\opt\quad\text{and}\quad\E_{S\sim\mu}[|S|]=k.
  \end{align*}
  \vspace{5mm}
  
  \textbf{Question.} Establish convexity of the following function:
  \begin{align*}
    g(x) &=\E_{S\sim\mu}[f(S)\mid |S|=x] =
           \frac{\Pr_{S\sim\PB(p)}[|S|=x]}{\Pr_{S\sim\mu}[|S|=x]},
  \end{align*}
  where $\PB(p)$ is the Poisson Binomial distribution for $p_1,...,p_n$.
\end{frame}


\begin{frame}
  \frametitle{D-optimal design: special cases}
1. Suppose that $\A\succ \zero$ and all $p_i>0$. Denoting $\D=\diag(\frac{p_i}{1-p_i})$,
  \begin{align*}
    g(x) \propto \frac{e_x(\D)}{e_x(\D^{\frac12}(\I+\X\A^{-1}\X^\top)\D^{\frac12})}.
  \end{align*}
  
2.  Suppose that $\A=\zero$ and $p_i=k/n$ for all $i$. Then:
  \begin{align*}
    g(x)
    &=  \frac{\sum_{S:|S|=x}f(S)\mu(S)}{\sum_{S:|S|=x}\mu(S)}
    \\
    &=\frac{{n\choose x}(\frac kn)^x(1-\frac kn)^{n-x}}
      {{n-d \choose x-d}(\frac kn)^{x-d}(1-\frac
      kn)^{n-x}}\det\big(\tfrac kn\X^\top\X\big)^{-1}
    \\
&=\frac{{n\choose x}}{{n-d\choose x-d}}\det(\X^\top\X)^{-1}.
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Example: A-optimal design}
  For a matrix $\X\in\R^{n\times d}$ and a p.s.d.~matrix
  $\A\in\R^{d\times d}$, we define:
  \begin{align*}
   \hspace{-4mm} f(S) = \tr\big((\X_S^\top\X_S+\A)^{-1}\big)\quad\text{and}\quad
    \mu(S)=\det(\X_S^\top\X_S+\A)\cdot\prod_{i\in S}p_i\cdot\prod_{i\not\in S}(1-p_i).
  \end{align*}
  If $k\geq d_{\A}+\frac{d_{\A}}\epsilon$ for
  $d_{\A}=\tr(\X^\top\X(\X^\top\X+\A)^{-1})$, then there are $p_i$'s s.t.:
  \begin{align*}
    \E_{S\sim\mu}[f(S)]\leq(1+\epsilon)\cdot\opt\quad\text{and}\quad\E_{S\sim\mu}[|S|]=k.
  \end{align*}
  \vspace{5mm}
  
  \textbf{Question.} Establish convexity of the following function:
  \begin{align*}
    g(x) &=\E_{S\sim\mu}[f(S)\mid |S|=x] =
           \frac{\sum_{S:|S|=x}f(S)\mu(S)}{\sum_{S:|S|=x}\mu(S)}
  \end{align*}
\end{frame}

%\section{Connection between CSSP and A/C/V-optimal design}

\begin{frame}
  \frametitle{Connection between CSSP and A/C/V-optimal design}
  Consider an $n\times d$ design matrix $\X$ and objective:
  \begin{align*}
    f(S\mid \A,\C) = \tr\big((\X_S^\top\X_S+\A)^{-1}\C\big),
  \end{align*}
  for some $d\times d$ matrices $\A$ and $\C$. Note that:
  \begin{itemize}
  \item For $\C=\I$ we recover A-optimality,
  \item For $\C=\cb\cb^\top$ we recover C-optimality,
  \item For $\C=\X^\top\X$ we recover V-optimality.
  \end{itemize}
We observe that CSSP lies in the closure of this family of
  problems:
  \begin{align*}
    f(S\mid \lambda\I,\lambda\X^\top\X)
    &= \tr(\lambda\X(\X_S^\top\X_S+\lambda\I)^{-1}\X^\top)
    \\
    &=\tr\big(\X(\I-\X_S^\top(\X_S\X_S^\top+\lambda\I)^{-1}\X_S)\X^\top\big)
    \\
    &\underset{\lambda\rightarrow 0}{\longrightarrow}
     \tr\big(\X(\I-\P_S)\X^\top\big) = \|\X(\I-\P_S)\|_F^2,
  \end{align*}
  where $\P_S=\X_S^\top(\X_S\X_S^\top)^{-1}\X_S$ is the projection onto
  the span of rows indexed by $S$, i.e., the CSSP objective for $\X^\top$.
\end{frame}

\end{document}