ONE SENTENCE SUMMARY:
Modern machine learning has increasingly relied on black-box models which are difficult to reason with, but the paper "Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nystrom method," by M. Derezinski, R. Khanna, and M. W. Mahoney, quantifies what can be called the "cost of interpretability" for many machine learning models, showing that it is far smaller than was previously believed.
THREE HUNDRED WORD SUMMARY:
Interpretable data summarization is a core challenge in reasoning about large datasets and machine learning models.  Data summarization can be used, e.g., to select a representative subset of gene variants from a genetics dataset, or a collection of most informative documents from a text database.  When data are represented numerically, they are often described via matrices, in which case linear algebra suggests a natural (and in certain ways optimal) way of performing data summarization: find the principal components corresponding to the largest directions of variance.  These principal components work well for black-box models that are evaluated only in terms of prediction quality, but they are generally not interpretable in terms of the domain from which the data are drawn. They do not correspond to, say, a particular document or a gene variant, but rather a complex mixture of them.  Neural networks, on which the machine learning community increasingly relies, simply exacerbate this problem.  A long-standing challenge has been to find summaries of data which mimic the numerical properties of principal components and which are also interpretable. The added cost of this interpretability can be significant, as shown by prior worst-case analysis.  In our work, we exploit the inherent structure of data to show that, except for pathological examples which we characterize, the cost of interpretability is far smaller than suggested by prior worst-case analysis, and it is often negligible for real-world problems.  Our analysis reveals an intriguing new phenomenon, which we call the multiple-descent curve in subset selection.  This is a phase transition, observed through a spike in the cost of interpretability, which occurs when the data exhibit an underlying hierarchical structure.  The multiple-descent phenomenon can be observed in practice, but it can also be avoided by tuning model parameters.
