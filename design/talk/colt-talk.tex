
\documentclass{beamer}
%\beamertemplateshadingbackground{brown!70}{yellow!10}
\mode<presentation>
{
  %\usetheme{Warsaw}
  \usecolortheme{crane}
  % or ...

%  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}
\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{footline}[frame number]{}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
\usepackage[utf8]{inputenc}
\usetikzlibrary{patterns}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{colortbl}
%\usepackage{multicol}
\usepackage{cancel}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{relsize}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{forloop}% http://ctan.org/pkg/forloop
\newcounter{loopcntr}
\newcommand{\rpt}[2][1]{%
  \forloop{loopcntr}{0}{\value{loopcntr}<#1}{#2}%
}
%\pagestyle{plain}
%\input{defs2}
\input{../shortdefs}

\edef\polishl{\l}
\setlength{\columnsep}{0.7em}
\setlength{\columnseprule}{0mm}
\setlength{\arrayrulewidth}{1pt} 

\newcommand{\svr}[1]{{\textcolor{darkSilver}{#1}}}
\definecolor{brightyellow}{cmyk}{0,0,0.7,0.0}
\definecolor{lightyellow}{cmyk}{0,0,0.3,0.0}
\definecolor{lighteryellow}{cmyk}{0,0,0.1,0.0}
\definecolor{lightestyellow}{cmyk}{0,0,0.05,0.0}
\AtBeginSection[]
{
\begin{frame}<beamer>
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}
}


%  \fboxsep=3pt
% %\fboxsep=0mm%padding thickness
% \fboxrule=2pt%border thickness


\setkeys{Gin}{width=0.7\textwidth}

\title[]{Minimax experimental design:\\
\small Bridging the gap between statistical and worst-case approaches to least squares regression}

\author[]{\small \hspace{-0.2cm}Micha{\l} Derezi\'{n}ski, Ken Clarkson, Michael Mahoney,
  Manfred Warmuth\\
 ~\quad \includegraphics[width=0.1\textwidth]{../sty/Berkeley.png}\qquad\qquad
  \includegraphics[width=0.09\textwidth]{../sty/IBM.png}\qquad\quad\quad\includegraphics[width=0.1\textwidth]{../sty/Berkeley.png}\qquad\quad\includegraphics[width=0.125\textwidth,viewport=0 160 375 261,clip]{../sty/UCSC}~\includegraphics[width=0.09\textwidth] {../sty/Google.jpg}}
% \author[]{$\text{
%   \begin{tabular}{c@{\qquad}c@{\qquad}c@{\qquad}c}
%   Micha{\polishl} Derezi\'{n}ski
%   & Kenneth L.~Clarkson
%   & Michael W.~Mahoney
%   & Manfred K.~Warmuth\end{tabular}}$}
%   \\[1mm]
% \includegraphics[height=1.5em]{../sty/Berkeley.png}
%   &\includegraphics[height=1.5em]{../sty/IBM.png}
%   & \includegraphics[height=1.5em]{../sty/Berkeley.png}
%   & \includegraphics[height=1.5em,viewport=0 160 375 261,clip]{../sty/UCSC}
%       \hspace{1mm}
% 	\includegraphics[height=1.5em] {../sty/Google.jpg}
% \end{tabular}
%}
\date{
  COLT'19\\
  June 27, 2019}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

\linespread{1.3}

\begin{frame}
  \frametitle{Classical experimental design}
  Consider $n$ parameterized experiments:
  $\x_1,\dots,\x_n\in\R^d$.\\
  Each experiment has a \underline{hidden} random response $y_i$ such that:
  \begin{align*}
    y_i = \x_i^\top\w^* + \xi_i,\qquad \xi_i\sim\Nc(0,\sigma^2)
  \end{align*}
  \pause
\textbf{Goal:} Select $k\ll n$ experiments to best estimate $\w^*$
  \pause
\begin{columns}
\begin{column}{0.3\textwidth}
\\ \vspace{0.8cm}
Select $S=\{4,6,9\}$\\
\vspace{1cm}
Receive $y_4, y_6, y_9$
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
	\begin{tikzpicture}[scale=0.9]
          \draw [fill=brown!30] (-2,0) rectangle (0,3);
          \draw [color=black] (-2,2) -- (0,2);
          \draw (-2.25,2) node {\mbox{\footnotesize $\x_4^\top$}}; 
          \draw [color=black] (-2,1.5) -- (0,1.5);
          \draw (-2.25,1.5) node {\mbox{\footnotesize $\x_6^\top$}}; 
          \draw [color=black] (-2,0.5) -- (0,0.5);
          \draw (-2.25,0.5) node {\mbox{\footnotesize $\x_9^\top$}}; 
	   \draw (-3,3) node {fixed $\X$}; 
           \draw [decorate,decoration={brace}] (-2,3.1) -- (0,3.1);
          \draw (-1,3.4) node {\mbox{\fontsize{8}{8}\selectfont $d$}}; 
            \draw [color=lightgray,line width =0.5mm] (1,0) -- (1,3);
            \draw [color=lightgray] (0.75,3) node {$\y$};
            \draw (0.75,2) node {\mbox{\footnotesize $y_4$}}; 
            \draw (1,2) node {.}; 
            \draw[mark=*,mark size=1.5pt] plot coordinates{(1,2)};
            \draw (0.75,1.5) node {\mbox{\footnotesize $y_6$}}; 
            \draw (1,1.5) node {.}; 
            \draw[mark=*,mark size=1.5pt] plot coordinates{(1,1.5)};
            \draw (0.75,0.5) node {\mbox{\footnotesize $y_9$}}; 
            \draw[mark=*,mark size=1.5pt] plot coordinates{(1,.5)};
	\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\end{frame}

\begin{frame}
  \frametitle{A-optimal design}
  Find an unbiased estimator $\wbh$ with smallest
  \textit{mean squared error}:
  \begin{align*}
\text{minimize}\quad
    \underbrace{\E_{\wbh}\big[\|\wbh-\w^*\|^2\big]}_{\mathrm{MSE}[\wbh]}\quad\text{subject
    to}\quad\E\big[\wbh\big]=\w^*\ \ \forall_{\w^*}
  \end{align*}
  \pause
Given every $y_1,\dots,y_n$, the optimum is \textit{least squares}: $\wbh=\X^\dagger\y$
  \begin{align*}
\MSE{\X^\dagger\y}=\tr\big(\Var[\X^\dagger\y]\big)=\sigma^2\underbrace{\tr\big((\X^\top\X)^{-1}\big)}_{\phi}
  \end{align*}
  \pause
  \vspace{-4mm}
  
% \begin{center}\fcolorbox{reddishyellow}{lightyellow}{
%   \parbox{0.95\textwidth}{
  \begin{theorem}[based on \cite{avron-boutsidis13}]
   There is an experimental design $S$ of size
$k\leq\Blue{d+\phi/\epsilon}$ such that
$$\mathrm{MSE}[\X_S^\dagger\y_S]\leq \epsilon\cdot
\underbrace{\tr(\Var[\xib])}_{n\sigma^2}$$
\vspace{-2mm}
% }}\end{center}
\end{theorem}
\pause
\Red{
    Required assumption:\quad $y_i = \x_i^\top\w^* +
    \xi_i,\quad\xi_i\sim\Nc(0,\sigma^2)$
  }
\end{frame}

\begin{frame}
  \frametitle{What if $\xi_i$ is not $\Nc(0,\sigma^2)$?}
  \begin{align*}
    \y&\text{ - any random vector in $\R^n$ with finite second moment}
    \\
\onslide<2->{    \w^*_{\y|\X}&\defeq  \argmin_\w \E_\y \big[\|\X\w-\y\|^2\big]
}
    \\ \onslide<3->{\xib_{\y|\X}&\defeq \y-\X\w_{\y|\X}^*
\quad\text{- deviation from best linear predictor}}
  \end{align*}
  \pause\pause\pause\vspace{3mm}

Two special cases:\vspace{2mm}
\pause
\begin{enumerate}
\item Statistical regression:\quad\quad\
  $\E\big[\xib_{\y|\X}\big]=\zero$\quad (mean-zero noise)
  \pause
\item Worst-case regression:\quad $\Var\big[\xib_{\y|\X}\big]=\zero$ \quad
  (deterministic $\y$)
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Random experimental designs}
  \textbf{Statistical:}  \ \,Fixed $S$ is ok\\
  \textbf{Worst-case:} Fixed $S$ can be exploited by the
  adversary\pause
  \vspace{5mm}
  
\begin{definition}
We define a \underline{random experimental design} $(S,\wbh)$
of size $k$ as:
  \begin{enumerate}
    \item a random subset $S\subseteq \{1..n\}$ s.t.~$|S|\leq k$
    \item a jointly random function $\wbh:\R^{|S|}\rightarrow\R^d$
    \end{enumerate}
    \end{definition}
    \vspace{6mm}
    
% $\Wc_k(\X)$ - family of \textit{unbiased} random experimental designs
% $(S,\wbh)$ of size $k$:
%     \begin{align*}
%       \E_{S,\wbh,\y}\big[\wbh(\y_S)\big] = \w^*_{\y|\X}
%       \qquad \text{for all }\y\in\Fc_n
%     \end{align*}
\end{frame}

  \begin{frame}
    \frametitle{Experimental design with arbitrary responses}
    \begin{theorem}[main result]
For any $\epsilon>0$, there is a random experimental design $(S,\wbh)$
of size
\[k=O(\Blue{d}\,\Red{\log n}\Blue{\,+\,\phi/\epsilon}), \quad\text{where}\quad\phi=\tr\big((\X^\top\X)^{-1}\big),\]
such that for any random $\y$ we have
\begin{align*}
\text{(unbiasedness)}\quad  \E\big[\wbh(\y_S)\big]&= \w^*_{\y|\X},\\[3mm]
\mathrm{MSE}\big[\wbh(\y_S)\big] - \mathrm{MSE}\big[\X^\dagger\y\big]
  &\leq \epsilon\cdot 
  \E\big[\|\xib_{\y|\X}\|^2\big]%\quad\text{and}\quad k= O(d\log n + \phi/\epsilon).
\end{align*}
\end{theorem}
% \pause\vspace{4mm}

% \textit{Toy example:} \quad$\Var[\xib_{\y|\X}]=\sigma^2\I$,\quad\ 
% $\E[\xib_{\y|\X}]=\zero$
% \pause\vspace{3mm}
% \begin{enumerate}
%   \item $\E\big[\|\xib_{\y|\X}\|^2\big]
%     %\tr\big(\Var[\xib_{\y|\X}]\big) + \big\|\E[\xib_{\y|\X}]\big\|^2
%     =\tr\big(\Var[\xib_{\y|\X}]\big)$
%     \pause\vspace{1mm}
%   \item $\MSE{\X^\dagger\y} %= \sigma^2\phi    %\leq \sigma^2k\epsilon\ll
%     % \sigma^2n \epsilon
%     = \frac\phi n\cdot \tr\big(\Var[\xib_{\y|\X}]\big)$
%   \end{enumerate}
  % \pause\vspace{3mm}
  
  % So in this case $\mathrm{MSE}\big[\wbh(\y_S)\big]\leq
  % O(\epsilon) \cdot \tr\big(\Var[\xib_{\y|\X}]\big)$ as expected
\end{frame}

\begin{frame}
  \frametitle{Important special instances}
  \begin{enumerate}
  \item    \textit{Statistical regression:}\quad
    $\y = \X\w^*+\xib$,\quad $\E[\xib]=\zero$
    % \begin{align*}
    %   \mathrm{MSE}\big[\wbh(\y_S)\big] -
    %   \mathrm{MSE}\big[\X^\dagger\y\big]\leq \epsilon\cdot\tr\big(\Var[\xib]\big)
    % \end{align*}
    \pause\vspace{2mm}
    \begin{itemize}
    \item Weighted regression:\quad\ 
      $\Var[\xib]=\mathrm{diag}\big([\sigma_1^2,\dots,\sigma_n^2]\big)$\\[2mm]
      \pause
      \item Generalized regression: $\Var[\xib]$ is
        arbitrary
        % \pause
        % \item Bayesian regression: \quad \,$\w^*\sim\Nc(\zero,\I)$
        \end{itemize}
        \pause\vspace{5mm}
      \item \textit{Worst-case regression:}\quad $\y$ is any fixed vector in
        $\R^n$
%         \pause
%         \begin{align*}
%       \E_{S,\wbh}\big[\|\wbh(\y_S)-\w^*\|^2\big]\leq \epsilon\cdot
%           \|\y-\X\w^*\|^2
%         \end{align*}
% where $\w^*=\X^\dagger\y$
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Main result: proof outline}
  \pause
  \begin{enumerate}
  \item Unbiased estimators via volume sampling\\[4mm]\pause
  \item Error bounds via i.i.d.~importance sampling:\\[1mm]\pause
    \begin{enumerate}
    \item Leverage score sampling:\quad  $\Pr(i) \defeq \frac1d \x_i^\top(\X^\top\X)^{-1}\x_i$\\[1mm]\pause
    \item Inverse score sampling:\quad \ \,$\Pr(i)\defeq
     \frac1\phi\x_i^\top(\X^\top\X)^{-2}\x_i$\quad\Red{(new)}
    \end{enumerate}
  \end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Volume sampling}
  \begin{definition}
  Given a  full rank matrix  $\X\in\R^{n\times d}$ we define volume sampling
  $\Vol(\X)$ as a distribution over sets $S\subseteq [n]$ of size $d$:
  \vspace{-1mm}
  \begin{align*}
    \Pr(S) =
    \frac{\det(\X_S)^2}{\det(\X^\top\X)}.
  \end{align*}
\end{definition}\pause
\begin{columns}
  \begin{column}{0.59\textwidth}
$$
\Pr(S)\sim \!\!{\footnotesize
\begin{array}{l}
  \text{squared volume}\\
  \text{of the parallelepiped} \\
\text{spanned by $\{\x_i:i\!\in\! S\}$}
\end{array}
}
$$
{\footnotesize Computational cost: $O(\mathrm{nnz}(\X)\log n+d^4\log d)$}
\end{column}
\begin{column}{.35\textwidth}
\includegraphics[width=\textwidth]{../talk/figs/volume_simple}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
  \frametitle{Unbiased estimators via volume sampling}
  Under arbitrary response model, any i.i.d.~sampling is \underline{biased}
\begin{theorem}[\cite{correcting-bias}]
Volume sampling corrects the least squares bias of i.i.d.~sampling.
\end{theorem}\pause
\vspace{3mm}
Let $q=(q_1,\dots,q_n)$ be some i.i.d.~importance sampling.
\begin{align*}
\text{\footnotesize volume + i.i.d.}\quad
  &\overbrace{\x_{i_1},\mydots,\x_{i_d}}^{\sim\Vol(\X)},\
  \overbrace{\x_{i_{d+1}},\,\x_{i_{d+2}},\mydots,\,\x_{i_{k}}}^{\sim q^{k-d}}
\end{align*}
\pause
\begin{align*}
  \E\bigg[\argmin_\w\sum_{t=1}^k\frac1{q_{i_t}}(\x_{i_t}^\top\w-y_{i_t})^2\bigg]
  = \w_{\y|\X}^*
\end{align*}
\end{frame}

% \begin{frame}
%   \frametitle{Importance sampling for experimental design}
%   \begin{enumerate}
%   \item  \textit{Leverage score sampling}: $\Pr(i)=p_i^{\mathrm{lev}}
% \defeq \frac1d \x_i^\top(\X^\top\X)^{-1}\x_i$\\
% {\footnotesize A standard sampling method for worst-case linear regression.}
% \pause
% \vspace{5mm}

%   \item \textit{Inverse score sampling}: $\Pr(i)= p_i^{\mathrm{inv}}\defeq
%     \frac1\phi\x_i^\top(\X^\top\X)^{-2}\x_i$.\\
% {\footnotesize  A novel sampling technique essential
%     for achieving $O(\phi/\epsilon)$ sample size.}
%   \end{enumerate}
% \end{frame}

\begin{frame}
  \frametitle{Conclusions}

  \begin{enumerate}
  \item Experimental design with \underline{arbitrary responses}
    \pause
  \item Upper bound almost matches the Gaussian noise setting
  \end{enumerate}  \pause
  \textbf{Open problems:}
  \begin{enumerate}
  \item Eliminating the $\Red{\log n}$ factor
  \item Efficiently finding \underline{minimax} optimal designs:
  \end{enumerate}
  \begin{definition}
    Minimax optimal experimental design with unbiased estimators:
\begin{align*}
R_k^*(\X)&\defeq 
  \min_{(S,\wbh)}\ \max_{\y}\,\frac{\mathrm{MSE}\big[\wbh(\y_S)\big]-
     \mathrm{MSE}\big[\X^\dagger\y\big]
  }{\E\big[\|\xib_{\y|\X}\|^2\big]}
\end{align*}
\end{definition}
\end{frame}

\begin{frame}
  \frametitle{References}
  \footnotesize
\bibliographystyle{alpha}
\bibliography{../pap}
\end{frame}

\end{document}