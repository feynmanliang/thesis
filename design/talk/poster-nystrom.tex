% \documentclass[8pt,aspectratio=1610]{beamer}
\documentclass{beamer}
\usepackage[orientation=landscape, size=a2, scale=1.75]{beamerposter}
% \beamertemplateshadingbackground{brown!70}{yellow!10}
\mode<presentation>
{
  %\usetheme{Warsaw}
  \usecolortheme{crane}
  % or ...

  \setbeamercovered{transparent}
%    \setbeamercovered{invisible}
  % or whatever (possibly just delete it)
}
% \setbeamertemplate{navigation symbols}{}
% \setbeamertemplate{footline}[frame number]{}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
\usepackage[utf8]{inputenc}
\usetikzlibrary{patterns}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{colortbl}
%\usepackage{multicol}
\usepackage{cancel}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{relsize}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{forloop}% http://ctan.org/pkg/forloop
\newcounter{loopcntr}
\usepackage{tkz-euclide}
\usetkzobj{all}
\newcommand{\rpt}[2][1]{%
  \forloop{loopcntr}{0}{\value{loopcntr}<#1}{#2}%
}
%\pagestyle{plain}
%\input{defs2}
\input{../shortdefs}

\edef\polishl{\l}
\setlength{\columnsep}{0.7em}
\setlength{\columnseprule}{0mm}
\setlength{\arrayrulewidth}{1pt} 

\newcommand{\svr}[1]{{\textcolor{darkSilver}{#1}}}
\definecolor{brightyellow}{cmyk}{0,0,0.7,0.0}
\definecolor{lightyellow}{cmyk}{0,0,0.3,0.0}
\definecolor{lighteryellow}{cmyk}{0,0,0.1,0.0}
\definecolor{lightestyellow}{cmyk}{0,0,0.05,0.0}

%  \fboxsep=3pt
% %\fboxsep=0mm%padding thickness
% \fboxrule=2pt%border thickness


\setkeys{Gin}{width=0.7\textwidth}

% \title{Unbiased estimates for linear regression\\
% via volume sampling}
% \author[]{Micha{\polishl } Derezi\'{n}ski and Manfred Warmuth}

\begin{document}

\begin{frame}
  \frametitle{
    \centering\textrm{\textbf{\LARGE Improved guarantees
        and a multiple-descent curve}}\\
\centering\textrm{\textbf{\LARGE
  for Column Subset Selection and the Nystr\"om method}}\\[-2mm]
\textit{\large Micha{\polishl } Derezi\'{n}ski, \ Rajiv Khanna,
  \ Michael Mahoney\qquad UC Berkeley}\\[-5mm]
}
\begin{columns}
  \begin{column}{0.32\textwidth}

  \begin{block}{Interpretable dimensionality reduction}

  \underline{Example application}: \textit{Genetics}
  \begin{center}
    \includegraphics[width=\textwidth]{figs/snp.png}
  \end{center}


  \begin{itemize}
  \item \cite{paschou2007pca} (in \textit{PLOS Genetics}, 2007) \\[-1mm]
{\scriptsize\textit{``PCA-correlated SNPs for structure
    identification in worldwide human populations''}}
  \item  \cite{mahoney2009cur} (in \textit{Proc.~of the NAS}, 2009)\\[-1mm]
    \hspace{-5mm}{\scriptsize\textit{``CUR matrix decompositions for
        improved data analysis''}}
  \end{itemize}
\end{block}
    
 \begin{block}{Subset selection}
    \begin{columns}
    \begin{column}{.43\textwidth}
      \begin{center}
        \emph{Column Subset Selection} \\[1mm]
\begin{tikzpicture}[scale=.9]
  \draw (0,0) rectangle (8,4);
  \draw (5,.75) node {\small Data matrix $\A$};
    \draw[fill=blue!20] (1,0) rectangle (2.5,4);
    %\draw (-1,2) node {\mbox{$\A\ =$}};
    %\draw (6.5,2) node {\mbox{$\in\R^{m\times n}$}};
    \draw [decorate,decoration={brace}] (1,4.1) -- (2.5,4.1);
    \draw (1.75,4.6) node {\small $S$}; 
  \end{tikzpicture}
\end{center}
\vspace{6mm}

{\small
\begin{enumerate}
\item Feature selection
\item Dimensionality reduction
\item Interpretability
\end{enumerate}}
\end{column}
\begin{column}{.53\textwidth}
  \begin{center}
    \emph{Nystr\"om method}\\[1mm]
\begin{tikzpicture}[scale=1]
  \draw (0,0) rectangle (5,5);
  \draw (2.5,.75) node {\small Kernel matrix $\K$};
    \draw[fill=blue!20] (1,2.5) rectangle (2.5,4);      
    %\draw (-1,2) node {\mbox{$\K\ =$}};
    %\draw (6.5,2) node {\mbox{$\in\R^{n\times n}$}};
    \draw [decorate,decoration={brace}] (1,5.1) -- (2.5,5.1);
    \draw (1.75,5.6) node {\small $S$};
    \draw [decorate,decoration={brace}] (-.1,2.5) -- (-.1,4);
    \draw (-.55,3.25) node {\small $S$}; 
  \end{tikzpicture}  
\end{center}
{\small
\begin{enumerate}
\item Kernel machines
\item Gaussian Process regression
\item Independent Component Analysis
\end{enumerate}}
\end{column}
\end{columns}
\end{block}

 \begin{block}{Interpretable low-rank approximation}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{align*}
        \Er = \big\|\A - \mathrm{Proj}(\A)\big\|_F^2
      \end{align*}
      \emph{Optimal choice}:\\
      {\small Top $k$ \textit{principal components}}\\
      \Red{\small Not interpretable!}\\[1mm]
      
      \emph{Goal}:\\
      {\small Find best \textit{interpretable} choice}
    \end{column}
    \begin{column}{0.45\textwidth}
      \begin{center}

        \begin{tikzpicture}[scale=1]

        \draw (-3.2,-3.2) -- (3.2,3.2);
        \tkzDefPoint(0,1){A};         \tkzDefPoint(0.5,.5){A1}; \tkzDrawSegment[thick,red](A,A1);
        \tkzDefPoint(1.8,.5){B};     \tkzDefPoint(1.15,1.15){B1}; \tkzDrawSegment[thick,red](B,B1);
        \tkzDefPoint(2.5,1.5){C};   \tkzDefPoint(2,2){C1}; \tkzDrawSegment[thick,red](C,C1);
        \tkzDefPoint(-1.5,-.3){D}; \tkzDefPoint(-.9,-.9){D1}; \tkzDrawSegment[thick,red](D,D1);
        \tkzDefPoint(-2,-.3){E};    \tkzDefPoint(-1.15,-1.15){E1}; \tkzDrawSegment[thick,red](E,E1);
        \tkzDefPoint(1,1.5){F};  \tkzDefPoint(1.25,1.25){F1}; \tkzDrawSegment[thick,red](F,F1);
        \tkzDefPoint(.5,-.5){G};  \tkzDefPoint(0,0){G1}; \tkzDrawSegment[thick,red](G,G1);
        \tkzDefPoint(2,4.4/1.65){H};  \tkzDefPoint((2+4.4/1.65)/2,(2+4.4/1.65)/2){H1}; \tkzDrawSegment[thick,red](H,H1);
        \tkzDefPoint(-1,0){I};  \tkzDefPoint(-.5,-.5){I1}; \tkzDrawSegment[thick,red](I,I1);
        \tkzDefPoint(1.1,2.5){J};  \tkzDefPoint(1.8,1.8){J1}; \tkzDrawSegment[thick,red](J,J1);
        \tkzDefPoint(-1.32,-1.76){K};  \tkzDefPoint(-1.54,-1.54){K1}; \tkzDrawSegment[thick,red](K,K1);
        \tkzDefPoint(-1.5,-2.5){L};  \tkzDefPoint(-2,-2){L1}; \tkzDrawSegment[thick,red](L,L1);


            \draw [thick,blue] (-3.3/1.4,-4.4/1.4) -- (3.3/1.4,4.4/1.4);
            % \tkzDrawSegment[thick,red](H,K);
            \foreach \n in {H,K} \node at (\n)[circle,fill=cyan,inner
            sep=2.25pt]{};


        \foreach \n in {A,B,C,D,E,F,G,H,I,J,K,L} \node at (\n)[circle,fill=blue,inner
        sep=1.5pt]{};
        \foreach \n in {A1,B1,C1,D1,E1,F1,G1,H1,I1,J1,K1,L1} \node at (\n)[circle,fill=black,inner
        sep=1pt]{};        

      \end{tikzpicture}   
    \end{center}
  \end{column}
\end{columns}
\vspace{5mm}

\underline{Cost of Interpretability}:\\[-11mm]

\begin{align*}
  \text{Approximation factor}\quad=\quad
  \frac{\Er(\text{best subset $S$ of size $k$})}{\Er(\text{top $k$
  principal components})}
\end{align*}

\end{block}

  \end{column}


  \begin{column}{0.32\textwidth}

    
    \begin{block}{Multiple-descent curve}
        \begin{align*}
\text{\underline{Prior work}~\cite{pca-volume-sampling}:}\quad
    \underbrace{\text{Approximation
    factor}\quad
\leq\quad k+1}_{\text{worst-case optimal!}} %\qquad\text{for size $k$}
  \end{align*}
  We go beyond worst case analysis!\vspace{2mm}
  
    \includegraphics[width=\textwidth]{../figs/nystrom/nystrom-bounds}

    \vspace{-4mm}    
    \begin{align*}
      \text{sharp spectral drop}\ &\Longrightarrow\ \text{large spike}\\[2mm]
      \text{smooth spectral decay}\ &\Longrightarrow\ \text{no spikes}
    \end{align*}

    
  \end{block}

  
  \begin{block}{Connection to double descent}
  \emph{Double descent}: Exhibited by generalization error \cite{BHMM19}
  \vspace{1cm}
  
  \begin{columns}
    \begin{column}{.6\textwidth}
      ``Classical'' ML: \hfill\textit{parameters} $\ll$ \textit{data}\\
    ``Modern'' ML: \hfill \textit{parameters} $\gg$ \textit{data}
\end{column}
\begin{column}{.35\textwidth}
\includegraphics[width=\textwidth]{figs/descent-intro-nice}
\end{column}
\end{columns}
\vspace{1cm}

\emph{Connections}:
\begin{enumerate}
\item Correlated with the condition number of the data \cite{double-descent-condition}
\item Multiple peaks are possible
  \cite{liang2020multiple,BLLT19_TR}
\item  Similar techniques can be used for the analysis \cite{surrogate-design}\\[-1mm]
{  \footnotesize\textit{``Exact expressions for
    double descent...''}, NeurIPS'20.}
\end{enumerate}
\vspace{5mm}
  
\end{block}



\end{column}


\begin{column}{.32\textwidth}


\begin{block}{Improved guarantees under smooth spectral decay}
Smooth spectral decay \ $\Longrightarrow$ \ no spikes!
\vspace{5mm}

\begin{enumerate}
\item 
  \emph{Polynomial decay}:\quad $i$th singular
  value \ $\asymp\ i^{-p}$
  \vspace{-2mm}
  \begin{align*}
    \text{Approximation factor}\ \leq\ O(1+p)\quad\text{for all $k$}
  \end{align*}
  \vspace{0mm}
  
\item 
  \emph{Exponential decay}:\ $i$th singular
    value \ $\asymp\ (1-\delta)^i$
    \vspace{-2mm}
    \begin{align*}
    \text{Approximation factor}\ \leq\ O(1+\delta k)\quad\text{for all $k$}
    \end{align*}
\end{enumerate}
\end{block}

  \begin{block}{\underline{Method}: Determinantal Point Processes (DPPs)}
  \emph{Non-i.i.d.}~randomized selection of column subset $S$\\[2mm]

  \emph{Negative correlation}: $\Pr(i\in S\mid j\in S) < \Pr(i\in S)$
  \vspace{-2mm}
  
\begin{center}
  \includegraphics[width=0.6\textwidth]{../figs/gue.png}
  \vspace{-3mm}
  
  \small  i.i.d.~(left) versus DPP (right)%
\end{center}
\vspace{-2mm}
\begin{itemize}
\item \emph{Fast algorithms}: \cite{alpha-dpp} (NeurIPS'20)\\[-1mm]
  {\footnotesize\textit{``Sampling
      from a $k$-DPP without looking at all items''}}
  \item \emph{Learn more}: \cite{dpps-in-randnla}
    (Notices of the AMS)\\[-1mm]
    {\footnotesize\textit{``Determinantal point processes in randomized numerical
      linear algebra''}}
\end{itemize}
%\let\thefootnote\relax\footnotetext{Image from \cite{dpp-ml}}  
\end{block}


  \begin{block}{Multiple-descent in real-world subset selection}
  \emph{Kernel}: Gaussian RBF,
  $\langle\a_i,\a_j\rangle_\text{K}=\exp(-\|\a_i\!-\!\a_j\|^2/\sigma^2)$\\[3mm]
  
  \centering
  \includegraphics[width=0.45\textwidth]{../figs/nystrom/rbf-bodyfat-double}
  \hspace{5mm}
  \includegraphics[width=0.45\textwidth]{../figs/nystrom/rbf-eunite2001-double}
  
  % \let\thefootnote\relax\footnotetext{Datasets from the Libsvm repository \cite{libsvm}}  
\end{block}

\end{column}
\end{columns}
\vspace{1cm}

\bibliographystyle{alpha}
  \bibliography{../pap}

\end{frame}


\end{document}
