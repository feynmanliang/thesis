\newif\ifisarxiv
\isarxivtrue

\ifisarxiv
\documentclass[11pt]{article}
\usepackage[round]{natbib}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathabx}
\usepackage{color}
\usepackage{cancel}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{xfrac}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mdframed}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={red!40!gray},
	citecolor={blue!40!gray},
	urlcolor={blue!70!gray}
}
\usepackage{cleveref}

\title{Exact expressions for double descent
and implicit regularization
\\
via surrogate random design}
  \author{%
          Micha{\l } Derezi\'{n}ski \\
  Department of Statistics\\
  University of California, Berkeley\\
  \texttt{mderezin@berkeley.edu}\\
  \and
  Feynman Liang \\
  Department of Statistics\\
  University of California, Berkeley\\
  \texttt{feynman@berkeley.edu}
  \and
   Michael W. Mahoney\\
  ICSI and Department of Statistics\\
  University of California, Berkeley\\
  \texttt{mmahoney@stat.berkeley.edu}
}

%  \newcommand{\jmlrBlackBox}{\rule{1.5ex}{1.5ex}}
  \providecommand{\BlackBox}{\rule{1.5ex}{1.5ex}}
%  \newcommand{\jmlrQED}{\hfill\jmlrBlackBox\par\bigskip}
  \renewenvironment{proof}%
  {%
   \par\noindent{\bfseries\upshape Proof\ }%
  }%
  {\hfill\BlackBox\par\bigskip}


\else
\documentclass[anon]{colt2020}

\title[Exact expressions for double descent and implicit
regularization]{Exact expressions for double descent and implicit
  regularization\\
  via surrogate random design}
\usepackage{times}

\coltauthor{%
  \Name{Micha{\l } Derezi\'{n}ski} \Email{mderezin@berkeley.edu}\\
  \addr Department of Statistics\\
  University of California, Berkeley\\
  \AND
  \Name{Feynman Liang} \Email{feynman@berkeley.edu}\\
  \addr Department of Statistics\\
  University of California, Berkeley\\
  \AND
  \Name{Michael W. Mahoney} \Email{mmahoney@stat.berkeley.edu}\\
  \addr ICSI and Department of Statistics\\
  University of California, Berkeley
}

\usepackage{float}
\usepackage{wrapfig}

\fi

\input{../shortdefs}



\begin{document}
\maketitle

\begin{abstract}
  Double descent refers to the phase transition that is exhibited by
  the generalization error of unregularized learning models when varying the ratio
  between the number of parameters and the number of training
  samples. The recent success of highly over-parameterized machine learning
  models such as deep neural networks has motivated a theoretical analysis of
  the double descent phenomenon in classical models such as linear
  regression which can also generalize well in the over-parameterized
  regime. We build on recent advances in Randomized Numerical Linear
  Algebra (RandNLA) to provide the first exact non-asymptotic
  expressions for double descent of the minimum norm linear
  estimator. Our approach involves constructing
  what we call a surrogate random design to replace the standard
  i.i.d.~design of the training sample. This surrogate design admits
  exact expressions for the mean squared error of the estimator while
  preserving the key properties of the standard design.
  We also establish an exact implicit regularization result for
  over-parameterized training samples. In particular, we show that, for
  the surrogate design, the implicit bias of the unregularized minimum
  norm estimator precisely corresponds to solving a ridge-regularized
  least squares problem on the population distribution.
\end{abstract}

\ifisarxiv\else
\begin{keywords}
  Least squares, determinantal point process,
  double descent, implicit regularization
\end{keywords}
\fi

\section{Introduction}

Classical statistical learning theory asserts that to achieve generalization
one must use training sample size that sufficiently exceeds the complexity of
the learning model, where the latter is typically represented by the number of
parameters \citep[or some related structural parameter; see][]{HFT09}.  In particular,
this seems to suggest the conventional wisdom that one should not use models
that fit the training data exactly.  However, modern machine learning practice
often seems to go against this intuition, using models with so many parameters
that the training data can be perfectly interpolated, in which case the
training error vanishes. It has been shown that models such as deep neural
networks, as well as certain so-called interpolating kernels and decision
trees, can generalize well in this regime. In particular,
\cite{BHMM19} empirically demonstrated a phase transition in generalization
performance of learning models which occurs at an \emph{interpolation
thershold}, i.e., a point where training error goes to zero (as one varies the
ratio between the model complexity and the sample size). Moving away from this
threshold in either direction tends to reduce the generalization error, leading
to the so-called \emph{double descent} curve.

To understand this
surprising phenomenon, in perhaps the simplest possible setting, we
study it in the context of linear or least squares regression.
Consider a full rank $n\times d$ data matrix $\X$ and a vector $\y$ of
responses corresponding to each of the $n$ data points (the rows of $\X$), where we wish to
find the best linear model $\X\w\approx \y$, parameterized by a
$d$-dimensional vector $\w$.
The simplest example of an estimator that has been shown to exhibit
the double descent phenomenon \citep{belkin2019two} is the
Moore-Penrose estimator, $\wbh=\X^\dagger\y$:
in the so-called over-determined regime, i.e., when $n>d$, it corresponds to the
least squares solution, i.e., $\argmin_{\w} \|\X\w-\y\|^2$; and in the
under-determined regime (also known as
over-parameterized or interpolating), i.e., when $n<d$, it
corresponds to the minimum norm solution to the linear system $\X\w=\y$.
Given the ubiquity of linear regression and the Moore-Penrose
solution, e.g., in kernel-based machine learning, studying the
performance of this estimator can shed some light on the effects of
over-parameterization/interpolation in machine learning more generally.
Of particular interest are results that are exact (i.e., not upper/lower bounds) and
non-asymptotic (i.e., for large but still finite $n$ and~$d$).

We build on methods from Randomized Numerical Linear Algebra (RandNLA) in order
to obtain \emph{exact non-asymptotic expressions} for the mean squared error
(MSE) of the Moore-Penrose estimator (see Theorem~\ref{t:mse}).  This provides
a precise characterization of the double descent phenomenon for perhaps the
simplest and most ubiquitous regression problem.  In obtaining these results,
we are able to provide precise formulas for the \emph{implicit regularization}
induced by minimum norm solutions of under-determined training samples,
relating it to classical ridge regularization (see Theorem~\ref{t:unbiased}).
This result has been observed empirically for RandNLA
methods~\citep{Mah-mat-rev_JRNL}, but it has also been shown in deep
learning~\citep{Ney17_TR} and machine learning~\citep{Mah12} more generally.  To
obtain our precise results, we use a somewhat non-standard random design, which
we term surrogate random design (see Section~\ref{s:determinantal} for a
detailed discussion), and which we expect will be of more general interest.
Informally, the goal of a surrogate random design is to modify an original
design to capture its main properties while being ``nicer'' in some useful way.
In Theorem~\ref{t:asymptotic} and Appendix \ref{sec:asymp-conj-details} we show, both
theoretically and empirically, that our surrogate design accurately preserves
the key properties of the original design when the data distribution is a
multivariate Gaussian.

\subsection{Main results: double descent and implicit regularization}

As the performance metric in our analysis, we use the \emph{mean
  squared error} (MSE), defined as
$\mathrm{MSE}[\wbh]=\E\big[\|\wbh-\w^*\|^2\big]$, where $\w^*$ is a fixed
underlying linear model of the responses.
In analyzing the MSE, we make the following standard assumption that
the response noise is homoscedastic with variance $\sigma^2$.
\begin{assumption}[Homoscedastic noise]\label{a:linear}
  Responses are $y(\x) = \x^\top\w^*+\xi$ where
$\xi\sim\Nc(0,\sigma^2)$.
\end{assumption}

\noindent
Our main result provides an exact expression for the MSE of the
Moore-Penrose estimator under our surrogate design denoted $\Xb\sim S_\mu^n$, where
$\mu$ is the $d$-variate distribution of the row vector $\x^\top$ and $n$ is the sample
size (details in Section~\ref{s:determinantal}). This surrogate is
used in place of the standard $n\times d$ random design $\X\sim\mu^n$, where $n$
data points (the rows of $\X$) are sampled independently from
$\mu$. Unlike for the standard design, our MSE formula is
fully expressible as a function of the covariance
matrix $\Sigmab_\mu=\E_\mu[\x\x^\top]$. To state our main result, we
need an additional minor assumption on $\mu$ which is satisfied by
most standard continuous distributions such as any multivariate
Gaussian with positive definite covariance matrix.
\begin{assumption}[General position]\label{a:general-position}
For $1\leq n \leq d$, if $\X\sim\mu^n$, then $\rank(\X)=n$ almost surely.
\end{assumption}

\noindent
Under Assumptions~\ref{a:linear} and~\ref{a:general-position}, we can
establish our first main result, stated as the following theorem, where
we use $\X^\dagger$ to denote the Moore-Penrose inverse of $\X$.
% , which gives an exact non-asymptotic
% expression for the MSE of the Moore-Penrose estimator  the
% surrogate design $S_\mu^n$.

\begin{theorem}[Exact non-asymptotic MSE]
\label{t:mse}
  If the response noise is homoscedastic % with variance~$\sigma^2$
  (Assumption~\ref{a:linear}) and $\mu$ is in
  general position (Assumption~\ref{a:general-position}), then for
  $\Xb\sim S_\mu^n$ (Definition \ref{d:surrogate}) and
  $\yb_i=y(\xbb_i)$, %we have
  \begin{align*}
    \MSE{\Xb^\dagger\ybb} =
    \begin{cases}
    \sigma^2\,\tr\big((\Sigmab_\mu+\lambda_n\I)^{-1}\big)\cdot
    \frac{1-\alpha_n}{d-n}\ +\
\frac{\w^{*\top}(\Sigmab_\mu+\lambda_n\I)^{-1}\w^*}
{\tr((\Sigmab_\mu+\lambda_n\I)^{-1})}\cdot (d-n),
&\text{for }n<d,\\
\sigma^2\, \tr(\Sigmab_\mu^{-1}),& \text{for }n=d,\\
\sigma^2\,\tr(\Sigmab_\mu^{-1})\cdot\frac{1-\beta_n}{n-d},&\text{for
}n>d,
\end{cases}
  \end{align*}
with $\lambda_n\geq 0$ defined by
  $n=\tr(\Sigmab_\mu (\Sigmab_\mu+\lambda_n\I)^{-1})$, \
  $\alpha_n=\det(\Sigmab_\mu(\Sigmab_\mu+\lambda_n\I)^{-1})$
  and $\beta_n=\ee^{d-n}$.
\end{theorem}
\begin{definition}
  We will use $\Mc(\Sigmab_\mu, \w^*,\sigma^2,n)$ to denote the above expressions
  for $\MSE{\Xb^\dagger\ybb}$.
\end{definition}

\begin{figure}[t]
\centering
\subfigure[%
  Surrogate MSE expressions (Theorem \ref{t:mse}) closely match
  numerical estimates even for non-isotropic
  features. Eigenvalue decay leads to a steeper
  descent curve in the under-determined regime ($n<d$).]{%
    \includegraphics[width=0.48\textwidth]{figs/descent-intro}
  }
\hfill
\subfigure[%
    The mean of the estimator $\X^\dagger\y$ exhibits
    shrinkage which closely matches the shrinkage of a
    ridge-regularized least squares optimum (theory lines), as characterized by
    Theorem \ref{t:unbiased}.]{%
      \includegraphics[width=0.48\textwidth]{figs/descent-shrinkage}
    }
\caption{Illustration of the main results for $d=100$ and
$\mu=\Nc(\zero,\Sigmab)$ where $\Sigmab$ is diagonal with
eigenvalues decaying exponentially and scaled so that
$\tr(\Sigmab^{-1})=d$. We use our surrogate
formulas to plot (a) the MSE (Theorem \ref{t:mse}) and (b) the norm of the expectation (Theorem
\ref{t:unbiased}) of the Moore-Penrose estimator (\emph{theory}
lines), accompanied by the empirical estimates based on the standard
i.i.d.~design (error bars are three times the standard error of the
mean). We consider three different condition numbers $\kappa$ of
$\Sigmab$, with \emph{isotropic} corresponding to $\kappa=1$,
i.e., $\Sigmab=\I$. We use $\sigma^2=1$ and
$\w^*=\frac1{\sqrt{d}}\one$.}
\vspace{-5mm}
\label{f:intro}
\end{figure}

\noindent
Proof of Theorem \ref{t:mse} is given in Appendix \ref{a:mse-proof}.
For illustration, we plot the MSE expressions in Figure~\ref{f:intro}a,
comparing them with empirical estimates of the true MSE under the
i.i.d.~design for a multivariate Gaussian distribution
$\mu=\Nc(\zero,\Sigmab)$ with several different covariance matrices $\Sigmab$. We keep the number of features $d$ fixed to
$100$ and vary the number of samples $n$, observing a double descent
peak at $n=d$. We observe that our theory aligns well with
the empirical estimates, whereas
previously, no such theory was available except for special
cases such as $\Sigmab=\I$ (more details in Theorem \ref{t:asymptotic}
and Appendices \ref{sec:proof-of-t-asymptotic} and \ref{sec:asymp-conj-details}). The plots
show that varying the spectral decay of $\Sigmab$ has a significant effect on the
shape of the curve in the under-determined regime. We use the
horizontal line to denote the MSE of the null estimator
$\mathrm{MSE}[\zero]=\|\w^*\|^2=1$. When the eigenvalues of $\Sigmab$
decay rapidly, then the Moore-Penrose estimator suffers less error
than the null estimator for some values of $n<d$, and the curve
exhibits a local optimum in this regime.

One important aspect of Theorem~\ref{t:mse} comes from the relationship between $n$ and the parameter $\lambda_n$, which together satisfy $n=\tr(\Sigmab_\mu (\Sigmab_\mu+\lambda_n\I)^{-1})$.
This expression is precisely the classical notion of \emph{effective
  dimension} for ridge regression regularized with
$\lambda_n$~\citep{ridge-leverage-scores}, and it arises here even though there is
no explicit ridge regularization in the problem being considered in
Theorem~\ref{t:mse}. 
The global solution to the ridge regression task (i.e., $\ell_2$-regularized
least squares) with parameter $\lambda$ is defined as:
\begin{align*}
\argmin_\w \Big\{\E_{\mu,y}\big[\big(\x^\top\w-y(\x)\big)^2\big]
    + \lambda\|\w\|^2\Big\}\ =\ (\Sigmab_\mu +
  \lambda\I)^{-1}\v_{\mu,y},\quad\text{where } \v_{\mu,y}=\E_{\mu,y}[y(\x)\,\x ].
\end{align*}
When Assumption \ref{a:linear} holds, then
$\v_{\mu,y}=\Sigmab_\mu\w^*$, however ridge-regularized least squares
is well-defined for much more general response models.
Our second result makes a direct connection between the (expectation
of the) unregularized minimum norm solution on the sample
and the global ridge-regularized solution.
While the under-determined regime (i.e., $n<d$) is of primary interest to us,
for completeness we state this result for arbitrary values of $n$ and $d$.
Note that, just like the definition of regularized least squares, this
theorem applies more generally than Theorem~\ref{t:mse}, in that it
does \emph{not} require the responses to follow any linear model as in
Assumption~\ref{a:linear} (proof in Appendix~\ref{s:unbiased-proof}). 
\begin{theorem}[Implicit regularization of Moore-Penrose estimator]
\label{t:unbiased}
For $\mu$ satisfying\footnote{The proof of Theorem \ref{t:unbiased}
  can be easily extended to probability measures $\mu$ that do not
  satisfy Assumption~\ref{a:general-position} (such as discrete distributions). We include this
  assumption here to simplify the presentation.} Assumption~\ref{a:general-position} and
  $y(\cdot)$ s.t.~$\v_{\mu,y}=\E_{\mu,y}[y(\x)\,\x]$ is well-defined, if
  $\Xb\sim S_\mu^n$ (Definition \ref{d:surrogate}) and $\yb_i=y(\xbb_i)$, then
  \begin{align*}
    \E\big[\Xb^\dagger\ybb\big] =
    \begin{cases}
       (\Sigmab_\mu + \lambda_n\I)^{-1}\v_{\mu,y} &\text{for }n<d,\\
        \Sigmab_\mu^{-1}\v_{\mu,y} &\text{for }n \ge d,
    \end{cases}
  \end{align*}
  where, as in Theorem \ref{t:mse}, $\lambda_n$ is such that the effective dimension
  $\tr(\Sigmab_\mu(\Sigmab_\mu+\lambda_n\I)^{-1})$ equals $n$.
\end{theorem}

\noindent
That is, when $n < d$, the Moore-Penrose estimator (which itself is
not regularized), computed on the
random training sample, in expectation equals the global ridge-regularized least
squares solution of the underlying regression
problem. Moreover, $\lambda_n$, i.e., the amount
of implicit $\ell_2$-regularization, is controlled by the degree of
over-parameterization in such a way as to ensure that $n$ becomes the ridge effective dimension
(a.k.a.~the effective degrees of freedom).

We illustrate this result in Figure
\ref{f:intro}b, plotting the norm of the expectation of the
Moore-Penrose estimator. As for the MSE, our surrogate theory aligns
well with the empirical estimates for i.i.d.~Gaussian designs, showing
that the shrinkage of the unregularized estimator in the
under-determined regime matches the implicit
ridge-regularization characterized by Theorem \ref{t:unbiased}. While the shrinkage
is a linear function of the sample size $n$ for isotropic features
(i.e., $\Sigmab=\I$), it
exhibits a non-linear behavior for other spectral decays.
Such \emph{implicit regularization} has been studied
previously~\citep[see, e.g.,][]{MO11-implementing, %PM11, GM14_ICML,
Mah12}; it has
been observed empirically for RandNLA sampling
algorithms~\citep{MMY15}; and it has also received attention more
generally within the context of neural networks~\citep{Ney17_TR}. While our implicit regularization result
is limited to the Moore-Penrose estimator, this new connection (and
others, described below) between the minimum norm solution of an unregularized
under-determined system and a ridge-regularized least squares solution
offers a simple interpretation for the implicit regularization
observed in modern machine learning architectures.

Our exact non-asymptotic expressions in Theorem~\ref{t:mse} and
our exact implicit regularization results in Theorem~\ref{t:unbiased}
are derived for the surrogate design, but Figure \ref{f:intro}
suggests that they accurately describe the MSE (up to lower order
terms) also under the standard i.i.d.~design $\X\sim\mu^n$,
particularly when $\mu$ is a multivariate Gaussian.
As a third result, we can verify this in the cases where there exist
known expressions for the MSE under the i.i.d.~design (standard
Gaussian for the under-determined setting, and
arbitrary Gaussian for the over-determined one).
We give the proof in Appendix~\ref{sec:proof-of-t-asymptotic}.
\begin{theorem}[Asymptotic consistency of surrogate design]
\label{t:asymptotic}
Let $\rho=n/d\neq 1$, $\X\sim \mu^n$
and $y_i=y(\x_i)$ satisfy Assumption~\ref{a:linear}. If $d\geq
c_\rho=\frac3{|1-\rho|}$ and
\begin{align*}
  \mu &= \begin{cases}
    \Nc(\zero,\I)&\text{when }n<d-1,\\
    \Nc(\zero,\Sigmab),\ \Sigmab\succ\zero &\text{when }n>d+1,
  \end{cases}
\end{align*}
then the absolute difference between surrogate
expressions and the true MSE is bounded as follows:
% &\text{if}\qquad  (1)\quad \rho<1\ \text{ and }\ \mu=\Nc(\zero,\I)\qquad\text{or}\qquad
%     (2)\quad \rho > 1\ \text{ and }\ \mu=\Nc(\zero,\Sigmab),\
%     \Sigmab\succ\zero,\\
\begin{align*}
\Big|\MSE{\X^\dagger\y}-\Mc(\Sigmab, \w^*,\sigma^2,n)\Big|\ \leq\
\frac{c_\rho}{d} \cdot \Mc(\Sigmab,\w^*,\sigma^2,n).
\end{align*}
\end{theorem}
\begin{remark}
For $n$ equal to $d-1$, $d$ or $d+1$, the true MSE under Gaussian
random design can be infinite, whereas the
surrogate MSE is finite and has a closed form expression.
\end{remark}

\noindent
Empirical estimates given in Figure
\ref{f:intro} suggest that the consistency of surrogate expressions
holds much more generally than it is stated above. Based on a detailed
empirical analysis described in Appendix \ref{sec:asymp-conj-details},
we conjecture that an asymptotic consistency 
result of the form similar to the statement of Theorem~\ref{t:asymptotic}
holds true in the under-determined regime without the assumption that
$\Sigmab=\I$ (see Conjectures \ref{c:wishart} and
\ref{c:projection}). In this case, no formula is known for 
$\MSE{\X^\dagger\y}$, whereas the expressions for the surrogate
Gaussian design naturally extend.

\subsection{Key techniques: surrogate designs and
  determinant preserving matrices}

The standard random design model for linear regression assumes that
each pair $(\x_i^\top,y_i)$ is drawn independently, where the row vector
$\x_i^\top$ comes from some $d$-variate distribution $\mu$ and $y_i=y(\x_i)$ is a random response variable drawn conditionaly on $\x_i$.
Precise theoretical analysis of under-determined regression in this
setting poses significant challenges, even in such special cases as
the Moore-Penrose estimator and a Gaussian data distribution $\mu$.
Rather than trying to directly analyze the usual i.i.d.~random design
$\X\sim\mu^n$ described above, we modify it slightly by introducing
the notion of a \emph{surrogate random design}, $\Xb\sim S_\mu^n$.
Informally, the goal of a surrogate random design is to modify an
original design to capture the main properties of the original design,
while being ``nicer'' for theoretical or empirical analysis.
In particular, here, we will modify the distribution of matrix $\X$ so
as to:
\begin{enumerate}
  \item closely preserve the behavior of the Moore-Penrose estimator
from the i.i.d.~design; and
  \item obtain exact expressions for double descent in terms of the
    mean squared error.
\end{enumerate}
A key element in the construction of our surrogate designs involves rescaling the measure $\X\sim\mu^n$ by the pseudo-determinant $\pdet(\X\X^\top)$, i.e., a product of the non-zero eigenvalues.
A similar type of determinantal design was suggested by
\cite{correcting-bias-journal}, but it was restricted there only to
$n\geq d$.
We broaden this definition by not only allowing the sample size to be
less than $d$, but also allowing it to be randomized.
A key property of our determinantal design matrix $\Xb$ is that
the expectation of a function $F(\Xb)$ can be, up to normalization,
expressed as:
\begin{align*}
  \E[F(\Xb)]\ \propto\ \E[\pdet(\X\X^\top)F(\X)],
  \end{align*}
where $\X\sim\mu^K$ and $K$ is a random variable (see details in Definition~\ref{d:det}).
Then, we define (in Definition~\ref{d:surrogate}) our surrogate design $S_\mu^n$ for each $n>0$ as a determinantal design with a carefully chosen random variable $K$, so that the expected sample size is equal to $n$ and so that it is possible to derive closed form expressions for the MSE.
We achieve this by using the Poisson distribution to construct the variable~$K$.

The key technical contribution that allows us to derive the MSE for determinantal designs is the concept of \emph{determinant preserving random matrices}, a notion that we expect to be useful more generally.
Specifically, in Section~\ref{s:dp} we define a class of $d\times d$ random
matrices $\A$ for which taking the determinant commutes with taking
the expectation, for the matrix itself and any of its square submatrices (see Definition~\ref{d:main}):
\begin{align*}
  \E\big[\!\det(\A_{\Ic,\Jc})\big] =
  \det\!\big(\E[\A_{\Ic,\Jc}]\big)\quad \text{for all }\Ic,\Jc\subseteq
  [d]\text{ s.t. }|\Ic|=|\Jc|.
\end{align*}
  Not all random matrices satisfy this property, however many
interesting and non-trivial examples can be found. Constructing
these examples is facilitated by the closure properties that this
class enjoys. In particular, if $\A$ and $\B$ are determinant
preserving and independent, then $\A+\B$ and $\A\B$ are also
determinant preserving (see Lemma \ref{t:ring}). We use these
techniques to prove a number of determinantal expectation
formulas needed in obtaining Theorems~\ref{t:mse}
and~\ref{t:unbiased}. For example, we show that if $\X\sim\mu^K$,
where $K$ is a Poisson random variable, then:
\begin{align*}
  \text{(Lemma \ref{l:poisson})}\quad  \E\big[\det(\X^\top\X)\big]
  &= \det\!\big(\E[\X^\top\X]\big),\\
\text{(Lemma \ref{l:normalization})}\quad  \E\big[\det(\X\X^\top)\big]
&= \ee^{-\E[K]}\det\!\big(\I + \E[\X^\top\X]\big).
\end{align*}

\section{Related work}
\label{s:related-work}

There is a large body of related work, which for simplicity we cluster into three groups.

\textbf{Double descent.}
The double descent phenomenon \cite[a term introduced by][]{BHMM19}
corresponds to the phase transition in the generalization error that
occurs when the ratio between the model complexity and the
sample size crosses the so-called interpolation threshold. It has been observed empirically
in a number of learning models, including neural networks
\citep{BHMM19,GJSx19_TR}, kernel methods \citep{BMM18_TR,BRT18_TR},
nearest neighbor models \citep{BHM18_TR}, and decision trees \citep{BHMM19}. The
theoretical analysis of double descent, and more broadly the generalization
properties of interpolating estimators, have primarily focused on various forms of
linear regression. The most comparable
to our work are \cite{BLLT19_TR,LR18_TR} and \cite{HMRT19_TR}, who provide
non-asymptotic upper/lower bounds and asymptotic formulas,
respectively, for the generalization error of the Moore-Penrose
estimator under essentially the same i.i.d.~random design setting as
ours. On the other hand, \cite{MVSS19_TR} provide bounds for the error
of the ideal linear interpolator (instead of the minimum norm
one). Note that while we analyze the classical mean squared error, many
works focus on the squared prediction error instead (some of them still
refer to it as the MSE).
Another line of literature deals with linear regression in the
so-called \emph{misspecified} setting, where the set of observed
features does not match the feature space in
which the response model is linear
\citep{belkin2019two,HMRT19_TR,Mit19_TR,MM19_TR}, e.g., when the
learner observes a random subset of $d$ features from a larger
population. This is an important distinction,
because it allows varying the model complexity by changing the number
of observed features while keeping the linear model fixed (see further
discussion in Section \ref{s:conclusions}). We believe
that our results can be extended to this important setting, and we leave
this as a direction for future work.

% Here are things to cite about double descent and related things.
% Original double descent papers:
% \cite{BHMM19}
% \cite{BMM18_TR}
% \cite{BRT18_TR}
% \cite{belkin2019two}
% Other double descent papers:
% \cite{AS17_TR}
% \cite{BLLT19_TR}
% \cite{BHM18_TR}
% \cite{BFF19_TR}
% \cite{COB18_TR}
% \cite{GJSx19_TR}
% \cite{HMRT19_TR}
% \cite{LR18_TR}
% \cite{MBB17_TR}
% \cite{MM19_TR}
% \cite{Mit19_TR}
% \cite{MVSS19_TR}
% \cite{NMBx18_TR}
% \cite{SKS18_TR}
% \cite{SGAx18_TR}
% \cite{WGSx19_TR}
% \michael{Say something about how we use MSE but others use one of a wide range of slightly different generalization metrics. Would you put in a draft, and then I can make a pass.}


\textbf{RandNLA.}
Randomized numerical linear algebra
\citep{DM16_CACM,RandNLA_PCMIchapter_TR} has
traditionally focused on obtaining algorithmic improvements for tasks
such as least squares and low-rank approximation via techniques that include
sketching \citep{sarlos-sketching} and i.i.d.~leverage score sampling
\citep{Drineas2006sampling}. However, there has been growing
interest in understanding the statistical properties of these
randomized methods \citep{MMY15,GarveshMahoney_JMLR}, for example
looking at the mean squared error of the least squares estimator
obtained via i.i.d.~subsampling under the standard linear response
model. Determinantal sampling methods (a.k.a.~volume sampling, or
determinantal point processes), which first found their way
into RandNLA in the context of low-rank approximation
\cite[][]{pca-volume-sampling}, have been recently shown to combine
strong worst-case guarantees with elegant statistical properties.
In particular, \cite{unbiased-estimates} showed that the
least-squares estimator subsampled via the so-called size $d$ volume
sampling (loosely corresponding to the special case of our surrogate design
$S_\mu^n$ where $n=d$) is an unbiased estimator that admits exact
formulas for both the expected square loss (a worst-case metric)
and the mean squared error (a statistical metric). These results were
developed further by
\cite{leveraged-volume-sampling,correcting-bias,minimax-experimental-design},
however they were still limited to the over-determined setting 
\cite[with the exception
of][who gave upper bounds on the mean squared error of the ridge estimator
under different determinantal
samplings]{regularized-volume-sampling,bayesian-experimental-design}.
Also in the over-determined setting, 
\cite{correcting-bias-journal} provided evidence for the fact that
determinantal rescaling can be used to modify the original data
distribution (particularly, a multivariate Gaussian) without a
significant distortion to the estimator, while making certain
statistical quantities expressible analytically. We take this
direction further by analyzing the unregularized least squares
estimator in the under-determined setting which is less well
understood, partly due to the presence of implicit regularization.

\textbf{Implicit regularization.}
The term implicit regularization typically refers to the notion that approximate
computation (e.g., rather than exactly minimizing a function $f$,
instead running an approximation algorithm to get an approximately
optimal solution) can implicitly lead to
statistical regularization (e.g., exactly minimizing an objective of
the form $f + \lambda g$, for some well-specified $\lambda$ and $g$).
See~\cite{MO11-implementing, PM11, GM14_ICML} and references therein
for early work on the topic; and see~\cite{Mah12} for an overview.
More recently, often motivated by neural networks, there has been work
on implicit regularization that typically considered SGD-based
optimization algorithms.
See, e.g., theoretical results on simplified
models~\citep{NTS14_TR,Ney17_TR,SHNx17_TR,GWBNx17,ACHL19,KBMM19_TR}
as well as extensive empirical and phenomenological results on state-of-the-art neural
network models~\citep{MM18_TR,MM19_HTSR_ICML}.
The implicit regularization observed by us is different in that it is
not caused by an inexact approximation algorithm (such as SGD) but rather by the
selection of one out of many exact solutions (e.g., the minimum norm
solution). In this context, most relevant are the
asymptotic results of~\cite{LJB19_TR} \cite[which used the asymptotic risk
results for ridge regression of][]{DW15_TR} and~\cite{KLS18_TR}.
Our non-asymptotic results are also related to recent work in
RandNLA on the expectation of the 
inverse~\citep{determinantal-averaging} and generalized
inverse~\citep{MDK19_TR} of a subsampled matrix.

\section{Surrogate random designs}
\label{s:determinantal}

In this section, we provide the definition of our surrogate random
design $S_\mu^n$, where $\mu$ is a $d$-variate probability measure and
$n$ is the sample size. This distribution is used in place
of the standard random design $\mu^n$ consisting of $n$ row vectors drawn
independently from $\mu$. Our surrogate design uses determinantal
rescaling to alter the joint distribution of the vectors so that
certain expected quantities (such as the mean squared error of the
Moore-Penrose estimator) can be expressed in a closed form.
We start by introducing notation.
%to be used throughout the rest of the paper.

%\paragraph{Preliminaries}
\textbf{Preliminaries.}
The set $\{1,...,n\}$ will be denoted by $[n]$.
For an $n\times n$ matrix $\A$, we use $\pdet(\A)$ to denote the pseudo-determinant of $\A$,
which is the product of non-zero eigenvalues. For index subsets $\Ic$
and $\Jc$, we use $\A_{\Ic,\Jc}$ to denote the submatrix of $\A$ with
rows indexed by $\Ic$ and columns indexed by $\Jc$. We may write
$\A_{\Ic,*}$ to indicate that we take a subset of rows. We use $\adj(\A)$ to
denote the adjugate of $\A$, defined as follows: the
$(i,j)$th entry of $\adj(\A)$ is
$(-1)^{i+j}\det(\A_{[n]\backslash\{j\},[n]\backslash\{i\}})$.
% We will
% use two useful identities related to the adjugate: (1)
% $\adj(\A)=\det(\A)\A^{-1}$ for invertible $\A$, and (2)
% $\det(\A+\u\v^\top)=\det(\A)+\v^\top\!\adj(\A)\u$.
% A formula called Sylvester's theorem
% relates the adjugate and the determinant: $\det(\A+\u\v^\top)=\det(\A)+\v^\top\!\adj(\A)\u$.
For a probability measure $\mu$ over $\R^d$, we use $\x^\top\!\sim\mu$
to denote a random row vector $\x^\top$ sampled according to this distibution.
We let $\X\sim\mu^k$ denote a $k\times d$ random matrix with rows
drawn i.i.d.~according to $\mu$, and the $i$th row is denoted as $\x_i^\top$.
We also let $\Sigmab_\mu=\E_{\mu}[\x\x^\top]$, where $\E_{\mu}$ refers to
the expectation with respect to $\x^\top\!\sim\mu$, assuming throughout that
$\Sigmab_\mu$ is well-defined and positive definite.
We use $\Poisson(\gamma)_{\leq a}$ as the Poisson distribution
restricted to $[0,a]$, whereas $\Poisson(\gamma)_{\geq a}$ is restricted
to $[a,\infty)$.
We also let $\#(\X)$ denote the number of rows of $\X$.

We now define a family of determinantal distributions over random matrices $\Xb$,
where not only the entries but also the number of rows is
randomized.
This randomized sample size is a crucial property of our
designs that enables our analysis.
% Our definition follows by
% expressing $\E[F(\Xb)]$ for real-valued functions $F:\bigcup_{k\geq 0}\R^{k\times
%   d}\rightarrow \R$ (the expectation may be undefined for some functions).
\begin{definition}\label{d:det}
  Let $\mu$ satisfy Assumption~\ref{a:general-position} and let $K$ be
  a random variable over non-negative integers. A determinantal design
    $\Xb\sim \Det(\mu,K)$ is a
distribution with the same domain as $\X\sim\mu^K$ such that~for any
event $E$ measurable w.r.t.~$\X$, we have
\begin{align*}
\Pr\big\{\Xb\in E\big\}\ = \frac{\E[\pdet(\X\X^\top)\one_{[\X\in E]}]}{\E[\pdet(\X\X^\top)]}.
\end{align*}
\end{definition}

\noindent
% Setting $F(\cdot)$ to 1, observe that the proportionality constant
% must be $1/\E[\pdet(\X\X^\top)]$.
The above definition can be
interpreted as rescaling the density function of $\mu^K$ by the
pseudo-determinant, and then renormalizing it.
We now construct our surrogate design $S_\mu^n$ by appropriately
selecting the random variable $K$.
The obvious choice of $K=n$ does \emph{not} result in simple closed form expressions for the MSE in the under-determined regime (i.e., $n<d$), which is the regime of primary interest to us.
Instead, we derive our random variables $K$ from the Poisson distribution.
\begin{definition}\label{d:surrogate}
For $\mu$ satisfying Assumption~\ref{a:general-position},
define surrogate design $S_\mu^n$ as $\Det(\mu,K)$ where:
\vspace{-2mm}
    \begin{enumerate}
\item if $n<d$, then $K\sim \Poisson(\gamma_n)_{\leq d}$ with
 $\gamma_n$ being the solution of
 $n=\tr(\Sigmab_\mu(\Sigmab_\mu+\frac1{\gamma_n}\I)^{-1})$,
 \vspace{-2mm}
\item if $n=d$, then we simply let $K=d$,
  \vspace{-2mm}
\item if $n>d$, then $K\sim\Poisson(\gamma_n)_{\geq d}$ with $\gamma_n=n-d$.
\end{enumerate}
\end{definition}

\noindent
Note that the under-determined case, i.e., $n<d$, is restricted to $K\leq d$ so that, under Assumption~\ref{a:general-position}, $\pdet(\X\X^\top)=\det(\X\X^\top)$ with probability 1.
On the other hand in the over-determined case, i.e., $n>d$, we have
$K\geq d$ so that $\pdet(\X\X^\top)=\det(\X^\top\X)$. In the special case
of $n=d=K$ both of these equations are satisfied: $\pdet(\X\X^\top)=\det(\X^\top\X)=\det(\X\X^\top)=\det(\X)^2$.

The first non-trivial property of the surrogate design $S_\mu^n$ is
that the expected sample size is in fact always equal to $n$, which we
prove in Appendix \ref{appx: proof-of-l-size}.
\begin{lemma} \label{l:size}
Let $\Xb\sim S_\mu^n$ for any $n>0$.
 Then, we have $\E[\#(\Xb)] = n$.
\end{lemma}

\noindent
 Our general template for computing expectations under
 a surrogate design $\Xb\sim\S_\mu^n$ is to use the following expressions based on the
i.i.d.~random design $\X\sim\mu^K$:
\begin{align}
  \E[F(\Xb)] &=\begin{cases}
    \frac{\E[\det(\X\X^\top)F(\X)]}{\E[\det(\X\X^\top)]}
   \quad K\sim\Poisson(\gamma_n)&\text{for }n<d,\\[2mm]
    \frac{\E[\det(\X)^2F(\X)]}{\E[\det(\X)^2]}\hspace{3mm}
    \quad K=d&\text{for }n=d,\\[2mm]
    \frac{\E[\det(\X^\top\X)F(\X)]}{\E[\det(\X^\top\X)]}
   \quad K\sim\Poisson(\gamma_n)&\text{for }n>d.
  \end{cases}\label{eq:cases}
\end{align}
These formulas follow from Definitions \ref{d:det} and
\ref{d:surrogate} because the determinants $\det(\X\X^\top)$ and
$\det(\X^\top\X)$ are non-zero precisely in the regimes $n\leq d$ and
$n\geq d$, respectively, which is why we can drop the restrictions on the
range of the Poisson distribution.
The normalization constants for computing the expectations
can be obtained using the following formulas: if $\X\sim\mu^K$ then
\begin{align*}
\text{(Lemma~\ref{l:normalization})}&&
\E\big[\det(\X\X^\top)\big]  &= \ee^{-\gamma_n}\det(\I +
\gamma_n\Sigmab_\mu)&&\text{for } K\sim\Poisson(\gamma_n),\ n<d,\\
  \text{(Lemma~\ref{l:cb})}&& \E\big[\det(\X)^2\big] &=
  d!\det(\Sigmab_\mu),&&\text{for } K=n=d,\\
  \text{(Lemma~\ref{l:poisson})}&& \E\big[\det(\X^\top\X)\big]
&=\det(\gamma_n\Sigmab_\mu),&&  \text{for }K\sim\Poisson(\gamma_n),\ n>d.
\end{align*}
% \begin{remark}
% We will use $Z_\mu^n$ as a shorthand for the above normalization constants.
% \end{remark}
% For the case of $n=d$, the normalization constant can be found in the
% literature \cite{expected-generalized-variance,correcting-bias}:
We prove Lemmas \ref{l:poisson} and \ref{l:normalization} in Section
\ref{s:dp} by introducing the
concept of determinant preserving random matrices.
% The lemmas play a
% crucial role in deriving a number of new expectation formulas for the
% under- and over-determined surrogate designs that we use to prove Theorems~\ref{t:mse}
% and~\ref{t:unbiased} in Section~\ref{s:expectations}.
On the other hand, Lemma  \ref{l:cb} and the design $S_\mu^d$ can be found in the
literature \citep{expected-generalized-variance,correcting-bias-journal},
and we will rely on those known results in this case.
% Importantly, the $n=d$ case offers a continuous transition between the under-
% and over-determined regimes because the distribution $S_\mu^n$
% converges to $S_\mu^d$ when $n$ approaches $d$ from above and
% below.

We now highlight some key expectation formulas for surrogate designs, which are used to
derive the MSE expressions from Theorem \ref{t:mse}. A standard decomposition of
the MSE yields:% for the Moore-Penrose estimator yields:
\begin{align}
  \MSE{\Xb^\dagger\ybb}
  &= \E\big[\|\Xb^\dagger(\Xb\w^*+\xib)-\w^*\|^2\big]
  =\sigma^2\E\big[\tr\big((\Xb^\top\Xb)^{\dagger}\big)\big] +
    \w^{*\top}\E\big[\I-\Xb^\dagger\Xb\big]\w^*.\label{eq:mse-derivation}
\end{align}
Thus, our task is to find closed form expressions for the two
expectations above. If $n\geq d$, then the latter goes away because
$\Xb^\dagger\Xb$ is the projection onto the row-span of $\Xb$ so
when $\Xb$ has rank $d$ then $\I-\Xb^\dagger\Xb=\zero$. When
$n<d$, it is given in the following result (proof in
Appendix \ref{s:unbiased-proof}).
% can be obtained as a consequence of Lemma \ref{l:ridge-under}.
\begin{lemma}\label{l:proj}
If  $\Xb\sim S_\mu^n$ and $n<d$, then we have:
$\E\big[\I-\Xb^\dagger\Xb\big] = (\gamma_n\Sigmab_\mu+\I)^{-1}$.
\end{lemma}

\noindent
No such expectation formula is known for i.i.d.~designs, except when
$\mu$ is an isotropic Gaussian. In Appendix \ref{s:unbiased-proof}, we
also prove a generalization of Lemma \ref{l:proj} which is then used
to establish our implicit regularization result
(Theorem~\ref{t:unbiased}). We next give an expectation formula 
for the trace of the Moore-Penrose inverse of the covariance
matrix for a surrogate design (proof in Appendix \ref{a:mse-proof}).
\begin{lemma}\label{l:sqinv-all}
If  $\Xb\sim S_\mu^n$, then:\vspace{-2mm}
\begin{align*}
    \E\big[\tr\big((\Xb^\top\Xb)^{\dagger}\big)\big]
  &=
    \begin{cases}
\gamma_n\big(1-
\det\!\big((\tfrac1{\gamma_n}\I+\Sigmab_\mu)^{-1}\Sigmab_\mu\big)\big),
&\text{for }n<d,\\
\tr(\Sigmab_\mu^{-1}),
&\text{for }n=d,\\
\tr(\Sigmab_\mu^{-1})\,\frac{1-\ee^{-\gamma_n}}{\gamma_n},
&\text{for }n>d.
\end{cases}
\end{align*}
\end{lemma}

\noindent
% The case of $n=d$ in the above lemma was shown by
% \cite{correcting-bias-journal} in Theorem~2.12, whereas the case of $n>d$
% is closely related to their Theorem 2.9.
Our key contribution is the under-determined
regime (i.e., $n<d$), where we observe implicit regularization
given by $\lambda_n=\frac1{\gamma_n}$. Since $n =
\tr(\Sigmab_\mu(\Sigmab_\mu+\lambda_n\I)^{-1})=d-\lambda_n\tr((\Sigmab_\mu+\lambda_n\I)^{-1})$,
it follows that $\lambda_n=(d-n)/\tr((\Sigmab_\mu+\lambda_n\I)^{-1})$.
Combining this with Lemmas \ref{l:proj} and \ref{l:sqinv-all}, we
recover the surrogate MSE expression in Theorem~\ref{t:mse} (see details
in Appendix \ref{a:mse-proof}).


\section{Determinant preserving random matrices}
\label{s:dp}

In this section, we introduce the key tool for computing expectation formulas of matrix determinants.
It is used in our analysis of the surrogate design, and it should be of independent interest.

The key question motivating the following definition is: When does taking expectation commute with computing a determinant for a square random matrix?
\begin{definition}\label{d:main}
A random $d\times d$ matrix $\A$ is called determinant
  preserving (d.p.), if
\begin{align*}
  \E\big[\!\det(\A_{\Ic,\Jc})\big] =
  \det\!\big(\E[\A_{\Ic,\Jc}]\big)\quad \text{for all }\Ic,\Jc\subseteq
  [d]\text{ s.t. }|\Ic|=|\Jc|.
\end{align*}
\end{definition}

\noindent
Note that from the definition of an adjugate matrix (see preliminaries
in Section \ref{s:determinantal}) it immediately follows that if $\A$ is
determinant preserving then adjugate commutes with expectation for this matrix:
\begin{align}
  \E\big[\big(\!\adj(\A)\big)_{i,j}\big] &=
  \E\big[(-1)^{i+j}\det(\A_{[d]\backslash\{j\},[d]\backslash\{i\}})\big]\nonumber
  \\
&=(-1)^{i+j}\det\!\big(\E[\A_{[d]\backslash\{j\},[d]\backslash\{i\}}]\big)
  = \big(\!\adj(\E[\A])\big)_{i,j}.\label{eq:adj}
\end{align}
The adjugate is useful in our analysis because it connects the
determinant and the inverse via the formula
$\adj(\A)=\det(\A)\A^{-1}$, which holds for any invertible $\A$.
We next give a few simple examples to provide some intuition. First, note
that every $1\times 1$ random matrix is determinant preserving simply
because taking a determinant is an identity transfomation in one
dimension. Similarly, every fixed matrix is determinant preserving because
in this case taking the expectation is an identity
transformation. In all other cases, however, Definition \ref{d:main}
has to be verified more carefully. Further examples (positive and
negative) follow.
\begin{example}
If $\A$ has i.i.d. Gaussian entries $a_{ij}\sim\Nc(0,1)$, then
$\A$ is d.p.~because $\E[\det(\A)]=0$.
\end{example}

\noindent
In fact, it can be shown that all random matrices with independent entries
are determinant preserving. However, this is not a necessary condition.
\begin{example}\label{e:rank-1}
Let $\A = s\,\Z$, where $\Z$ is fixed with $\rank(\Z) = r$, and $s$
is a scalar random variable. Then for $|\Ic|=|\Jc|=r$ we have
\begin{align*}
  \E\big[\det(s\,\Z_{\Ic,\Jc})\big] &= \E[s^r]\det(\Z_{\Ic,\Jc})
                                      =\det\Big(\big(\E[s^r]\big)^{\frac1r}\,\Z_{\Ic,\Jc}\Big),
\end{align*}
  so if $r=1$ then $\A$ is determinant preserving, whereas if $r>1$
  and $\Var[s]>0$ then it is not.
\end{example}

\noindent
To construct more complex examples, we show that determinant preserving random matrices are
closed under addition and multiplication. The proof of this result is
an extension of an existing argument, given by
\cite{determinantal-averaging} in the proof of Lemma~7, for computing
the expected determinant of the sum of rank-1 random matrices (proof in Appendix \ref{a:dp}). 
\begin{lemma}[Closure properties]\label{t:ring}
  If $\A$ and $\B$ are independent and determinant preserving, then:
  \begin{enumerate}
  \item $\A+\B$ is determinant preserving,
  \item $\A\B$ is determinant preserving.
    \end{enumerate}
\end{lemma}
% \begin{lemma}\label{t:ring}
%   If $\A,\B$ are independent and d.p.~then
%   $\A+\B$ and $\A\B$ are also determinant preserving.
% \end{lemma}

\noindent
Next, we introduce another important class of d.p.~matrices:
a sum of i.i.d.~rank-1 random matrices with the number of
i.i.d.~samples being a Poisson random variable. Our use of the Poisson
distribution is crucial for the below result to hold. It is an
extension of an expectation formula given by \cite{dpp-intermediate}
for sampling from discrete distributions (proof in Appendix \ref{a:dp}).
\begin{lemma}\label{l:poisson}
If $K$ is a Poisson random variable and $\A,\B$ are random $K\times d$
matrices whose rows  are sampled as an i.i.d.~sequence of joint pairs of
random vectors, then $\A^\top\B$ is d.p., and so:
\begin{align*}
  \E\big[\det(\A^\top\B)\big] &= \det\!\big(\E[\A^\top\B]\big).
  \end{align*}
\end{lemma}

\noindent
Finally, we show the expectation formula needed for obtaining the
normalization constant of the under-determined surrogate design, given
in \eqref{eq:cases}.
The below result is more general than the normalization constant
requires, because it allows the matrices $\A$ and $\B$ to be different
(the constant is obtained by setting $\A=\B=\X\sim\mu^K$).
In fact, we use this more general statement to show Theorems
\ref{t:mse} and~\ref{t:unbiased}. The proof uses
Lemmas \ref{t:ring} and \ref{l:poisson} (see Appendix \ref{a:dp}).
\begin{lemma}\label{l:normalization}
If $K$ is a Poisson random variable and $\A$, $\B$ are random $K\times d$
matrices whose rows  are sampled as an i.i.d.~sequence of joint pairs of
random vectors, then
\begin{align*}
  \E\big[\det(\A\B^\top)\big] &= \ee^{-\E[K]}\det\!\big(\I + \E[\B^\top\A]\big).
  \end{align*}
\end{lemma}


\section{Conclusions and open problems}
\label{s:conclusions}

We derived exact non-asymptotic expressions for the MSE of the
Moore-Penrose estimator in the standard regression task, reproducing
the double descent phenomenon as the sample size crosses between the
under- and over-determined regime. To achieve this, we modified the
standard i.i.d.~random design distribution using a determinantal
rescaling to obtain a surrogate design which admits exact MSE expressions,
while capturing the key properties of the i.i.d.~design. We
also provided a result that relates the expected value of the
Moore-Penrose estimator of a training sample in the under-determined regime (i.e., the
minimum norm solution) to the ridge-regularized least squares solution
for the population distribution, thereby providing an interpretation for the
implicit regularization resulting from over-parameterization.

\begin{wrapfigure}{r}{\ifisarxiv 0.5\else 0.47\fi\textwidth}
 \ifisarxiv\else \vspace{-.3cm}\fi
  \centering
 \includegraphics[width=.46\textwidth]{figs/descent-model-log}
\ifisarxiv\else \vspace{-2mm}\fi
  \caption{
Surrogate MSE as a function of $d/n$, with $n$
fixed to $100$ and varying $d$.}
  \label{f:model}
\ifisarxiv\else  \vspace{-.5cm}\fi
\end{wrapfigure}

An important technical issue is that, in this work, we focus on the classical \emph{well-specified}
linear regression task, where the underlying response model is linear
with respect to the observed feature space. A significant effort in
the related literature (see Section \ref{s:related-work}) has been
directed towards a number of \emph{misspecified} linear regression
tasks, where the set of $d$ observed features is different than the
set of $D$ features which define the linear model (typically, $d\ll
D$). Crucially, unlike in the well-specified task, here it is possible
to vary the number of observed features without changing the
underlying linear model.
Recent work \citep{HMRT19_TR} has compared how
varying the feature dimension affects the (asymptotic) generalization
error for both well-specified and misspecified tasks, however their
analysis was limited to certain special settings such as an
isotropic data distribution. As an additional
point of comparison, in Figure~\ref{f:model} we plot the MSE
expressions of Theorem~\ref{t:mse} for our well-specified setting when varying the
feature dimension~$d$. The model is chosen just like in
Figure~\ref{f:intro}, where the covariances $\Sigmab_\mu$ are diagonal
with condition number $\kappa$ and exponentially decaying spectrum
scaled so that $\tr(\Sigmab_\mu^{-1})=d$. We also use $\sigma^2=1$ and
$\w^*=\frac1{\sqrt{d}}\one$. Qualitatively, our plots follow the
trends outlined by \cite{HMRT19_TR} for the isotropic case (i.e., $\kappa=1$),
but the spectral decay of the covariance matrix (captured by our new
MSE expressions) does have a significant effect on the descent curve
in the under-determined regime. Note that the plots achieve their
minimum as $d$ goes to zero because in the well-specified task as
the complexity of the prediction model decreases, so does the complexity of the
true response model. Nevertheless,
we observe generalization in the under-determined regime,
as seen by the fact that the MSE curve goes below the error of the
null estimator, $\mathrm{MSE}[\zero]=1$.


Our work opens up a number of new directions for future research.
This includes extending our surrogate analysis to the
misspecified linear regression discussed above.
Also, it remains open whether the analysis we provided for the mean
squared error can be reproduced in the context of mean squared
\emph{prediction} error, which is relevant in many machine learning
tasks. Finally, while Theorem \ref{t:asymptotic} states that our surrogate
expressions for the MSE under certain Gaussian designs are
asymptotically consistent with the multiplicative error rate of
$O(1/d)$, we believe that this fact extends to
the setting not covered by the theorem: under-determined regime (i.e.,
$n<d$) with a non-isotropic Gaussian distribution (i.e., $\Sigmab\neq
\I$). We break down our analysis into verifying two
conjectures which are of independent interest to multivariate
Gaussian analysis. The first conjecture addresses the first term in
the MSE derivation \eqref{eq:mse-derivation} and postulates an asymptotically
consistent formula for the expected Moore-Penrose inverse
of the pseudo-Wishart distribution. Matrix
$\W\sim\Pc\Wc(\Sigmab,n)$ is distributed according to the pseudo-Wishart
distribution with $n<d$ degrees of freedom \citep{srivastava1979introduction} if it can be written as
$\W=\X^\top\X$, where $\X\sim\Nc_{n,d}(\zero, \I_n \otimes \Sigmab)$
is the matrix-variate Normal \citep{gupta2018matrix}, i.e., an
i.i.d.~Gaussian design.
% we call it a pseudo-Wishart 
% and write $\W \sim \Pc\Wc(\Sigmab, n)$
% if $n<d$ but still
\ifisarxiv\pagebreak\fi
\begin{conjecture}[Moore-Penrose inverse of pseudo-Wishart]\label{c:wishart}
  Fix $n/d<1$ and let $\W\sim\Pc\Wc(\Sigmab,n )$, where
$\Sigmab$ is $d\times d$ positive definite with condition number bounded by a constant.
Then:
\begin{align}
\bigg|\frac{\E\big[\tr(\W^\dagger)\big]}{\Vc(\Sigmab,n)} -1\bigg|=
  O(1/d)\qquad\text{for}\quad\Vc(\Sigmab,n)=\frac{1-\alpha_n}{\lambda_n},\label{eq:wishart}
\end{align}
where $\lambda_n\geq 0$ satisfies $n=\tr(\Sigmab(\Sigmab+\lambda_n\I)^{-1})$ and
$\alpha_n=\det(\Sigmab(\Sigmab+\lambda_n\I)^{-1})$.
\end{conjecture}

\noindent
Our second conjecture involves the projection onto the
orthogonal complement of a Gaussian sample $\X$, i.e., the matrix
$\I-\X^\dagger\X$, and addresses
the second term in the MSE derivation \eqref{eq:mse-derivation}.
\begin{conjecture}[Gaussian orthogonal projection]\label{c:projection}
  Fix $n/d<1$ and let $\X\sim \Nc_{n,d}(\zero,\I_n \otimes \Sigmab)$, where
$\Sigmab$ is $d\times d$ positive definite with condition number
bounded by a constant. Then:
\begin{align}
\sup_{\w\in\R^d\backslash\{\zero\}}\bigg|\frac{\w^\top\E[\I-\X^\dagger\X]\w}{\w^\top
  \Bc(\Sigmab,n)\w} - 1\bigg| = O(1/d)\qquad\text{for}\quad
  \Bc(\Sigmab,n) =
  \lambda_n(\Sigmab+\lambda_n\I)^{-1}.\label{eq:projection}
\end{align}
\end{conjecture}
% Note that the surrogate expression for the mean squared
% error can be written as:
% \[
% \Mc(\Sigmab_\mu,\w^*,\sigma^2,n)=\sigma^2 \Vc(\Sigmab_\mu,n)
% + \w^{*\top}\Bc(\Sigmab_\mu,n)\w^*.
% \]
% So, if the conjectures are true,
% this would immediately imply that the asymptotic consistency claim
% given in Theorem \ref{t:asymptotic} for
% the surrogate Gaussian MSE extends to
% the under-determined setting with arbitrary covariance $\Sigmab$.

\noindent
Recall that $\lambda_n=\frac {d-n}{\tr((\Sigmab+\lambda_n\I)^{-1})}$,
so our surrogate MSE is recovered as
$\sigma^2\Vc(\Sigmab,n)+\w^{*\top}\Bc(\Sigmab,n)\w^*$. 
% The expressions $\Vc(\Sigmab,n)$ and $\Bc(\Sigmab,n)$ and recover
% surrogate MSE from Theorem \ref{t:mse} since $\lambda_n=\frac{\tr((\Sigmab+\lambda_n)^{-1})}{d-n}$.
Conjectures \ref{c:wishart} and \ref{c:projection} provide new insights into classical matrix-variate
distributions with extensive literature dedicated to them \citep[see,
e.g.,][]{chikuse1990matrix,cook2011}. We discuss 
this further in Appendix \ref{sec:asymp-conj-details},
where we also provide detailed empirical evidence supporting these claims.

% In Appendix \ref{sec:asymp-conj-details} we provide further discussion
% and detailed empirical evidence for these conejctures.
% We believe that addressing these conjecture will have a broader
% impact on our understanding of non-isotropic Gaussian designs. 




% In addition, in Section \ref{s:asymptotic}, we provided two conjectures, the goal of which is to bound the difference between the results obtained for our surrogate designs and the true values corresponding to the i.i.d.~design, in the case of multivariate Gaussians.


\paragraph{Acknowledgements.}
We would like to acknowledge ARO, DARPA, NSF, ONR, and GFSD for providing
partial support of this work.

\ifisarxiv\bibliographystyle{plainnat}\fi
\bibliography{../pap}

\appendix

\input{appendix}

\end{document}
