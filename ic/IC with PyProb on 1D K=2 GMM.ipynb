{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprob\n",
    "from pyprob import Model\n",
    "from pyprob.distributions import Normal, Mixture\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference compilation on 1D 2-cluster GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 1-dimensional Gaussian Mixture model where:\n",
    " * Number of components $K$ fixed\n",
    " * Component means $\\mu_i \\sim N(0, 100)$ and standard deviation $\\sigma_i = 0.1$ for all $i \\in [K]$\n",
    " * For each observation $\\texttt{obs}_i$ $(i \\in [\\texttt{num_obs}])$:\n",
    "     * Component membership $c_i \\sim \\text{Unif}([K])$\n",
    "     * Conditional distribution $\\texttt{obs}_i \\mid c_i = j \\sim N(\\mu_j, 0.1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(Model):\n",
    "    def __init__(self, num_obs, K):\n",
    "        super().__init__(name='Gaussian mixture model')\n",
    "        self.K = K\n",
    "        self.num_obs = num_obs\n",
    "\n",
    "    def forward(self):\n",
    "        mus = [pyprob.sample(Normal(0, 100)) for _ in range(self.K)]\n",
    "        probs = [1./self.K for _ in range(self.K)]\n",
    "        likelihood = Mixture(\n",
    "            [Normal(mus[i], 0.1) for i in range(self.K)],\n",
    "            probs=probs)\n",
    "        \n",
    "        for i in range(self.num_obs):\n",
    "            pyprob.observe(likelihood, name=f'obs{i}')\n",
    "\n",
    "        return torch.tensor(mus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of observations is fixed during model specification and must be the same during inference. This is a current limitation of inference compilation in PyProb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate this model with $K=2$ components and $4$ data points, and then lets sample the prior and compute an empirical mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent  | Time remain.| Progress             | Trace     | Traces/sec\n",
      "0d:00:00:00 | 0d:00:00:00 | #################### | 1000/1000 | 2,475.39       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.7497, 1.6490])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GMM(num_obs=4, K=2)\n",
    "prior = model.prior_results(num_traces=1000)\n",
    "prior.expectation(lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the empirical mean is close to the true mean of the prior $N(0, 100)$ over $\\mu_i$s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's demonstrate how we can perform inference compilation. In the following snippet, we:\n",
    " * Sample the model for 20000 execution traces\n",
    " * Use a feedforward neural network to embed it into a 16-dimensional vector\n",
    " * Train a LSTM + FFW NN + Random Variable specific proposal distribution to maximize the probability of the sample traces\n",
    " \n",
    "This trained network is referred to as the \"inference compilation artifact,\" and is used during inference as an importance sampling proposer which accounts for the state present up until this point within an execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new inference network...\n",
      "Observable obs0x: reshape not specified, using shape torch.Size([]).\n",
      "Observable obs0x: using embedding dim torch.Size([16]).\n",
      "Observable obs0x: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable obs0x: embedding depth not specified, using the default 2.\n",
      "Observable obs1x: reshape not specified, using shape torch.Size([]).\n",
      "Observable obs1x: using embedding dim torch.Size([16]).\n",
      "Observable obs1x: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable obs1x: embedding depth not specified, using the default 2.\n",
      "Observable obs2x: reshape not specified, using shape torch.Size([]).\n",
      "Observable obs2x: using embedding dim torch.Size([16]).\n",
      "Observable obs2x: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable obs2x: embedding depth not specified, using the default 2.\n",
      "Observable obs3x: reshape not specified, using shape torch.Size([]).\n",
      "Observable obs3x: using embedding dim torch.Size([16]).\n",
      "Observable obs3x: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable obs3x: embedding depth not specified, using the default 2.\n",
      "Observe embedding dimension: 64\n",
      "Train. time | Epoch| Trace     | Init. loss| Min. loss | Curr. loss| T.since min | Learn.rate| Traces/sec\n",
      "New layers, address: 60__forward__?__Normal__1, distribution: Normal\n",
      "New layers, address: 60__forward__?__Normal__2, distribution: Normal\n",
      "Total addresses: 2, distribution types: 1, parameters: 1,790,326\n",
      "0d:00:00:58 | 1    | 19,968    | +1.19e+01 | +7.47e+00 | \u001b[31m+7.79e+00\u001b[0m | 0d:00:00:06 | +1.00e-03 | 312.2                               \n",
      "Stop condition reached. num_traces: 20000\n",
      "0d:00:00:59 | 1    | 20,032    | +1.19e+01 | +7.47e+00 | \u001b[31m+7.82e+00\u001b[0m | 0d:00:00:06 | +1.00e-03 | 269.4                               \n"
     ]
    }
   ],
   "source": [
    "model.learn_inference_network(\n",
    "        num_traces=20000,\n",
    "        observe_embeddings={f'obs{i}x' : {'dim' : 16} for i in range(model.num_obs)},\n",
    "        inference_network=pyprob.InferenceNetwork.LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the inference network, we can use it to perform posterior importance sampling. Below, we condition on some observed data, draw 200 importance samples using our trained inference network, and finally compute a posterior mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent  | Time remain.| Progress             | Trace   | Traces/sec\n",
      "0d:00:00:01 | 0d:00:00:00 | #################### | 200/200 | 156.79       \n",
      "tensor([[ 99.2688],\n",
      "        [-99.9758]])\n"
     ]
    }
   ],
   "source": [
    "observed_list = [\n",
    "    100,\n",
    "    -100,\n",
    "    100,\n",
    "    -100,\n",
    "]\n",
    "\n",
    "posterior = model.posterior_results(\n",
    "    num_traces=200,\n",
    "    inference_engine=pyprob.InferenceEngine.IMPORTANCE_SAMPLING_WITH_INFERENCE_NETWORK, # specify which inference engine to use\n",
    "    observe={\n",
    "        **{f'obs{i}': observed_list[i] for i in range(model.num_obs)},\n",
    "    })\n",
    "print(posterior.expectation(lambda x: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the posterior means $\\{\\mu_0, \\mu_0\\} \\approx \\{100, -100\\}$. Because our prior on component means $\\mu_i$ is quite diffuse (with standard deviation $100$), we expect the posterior to be quite close in shape to the likelihood. Since our data is exactly at $\\pm 100$, these results make sense and suggest IC is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
